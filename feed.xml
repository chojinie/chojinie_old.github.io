<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://chojinie.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://chojinie.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-05-23T05:14:29+00:00</updated><id>https://chojinie.github.io/feed.xml</id><title type="html">Ride the waves</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Computer Vision_Human Pose Estimation</title><link href="https://chojinie.github.io/blog/2023/human-pose-estimation/" rel="alternate" type="text/html" title="Computer Vision_Human Pose Estimation" /><published>2023-05-23T00:00:00+00:00</published><updated>2023-05-23T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/human-pose-estimation</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/human-pose-estimation/"><![CDATA[<h2 id="글에-들어가기-앞서">글에 들어가기 앞서..</h2>

<p>우선, viso.ai에 잘 정리가 되어 있는 Human Pose Estimation with Deep Learning - Ultimate Overview in 2023을 읽은 것을 시작으로 해당분야의 정리 및 논문 탐색을 해보려 합니다.- <a href="https://viso.ai/deep-learning/pose-estimation-ultimate-overview/">viso.ai/deep-learning 관련 글</a></p>

<p>해당 기고문은 가장 최근의 발전된 pose estimation algorithm과 AI Vision 기술 그리고 이들의 application과 use case 그리고 한계점에 대해 전반적으로 기술하고 있기 때문에 overview에 적합하다고 생각됩니다.</p>

<h2 id="pose-estimation이란">Pose Estimation이란?</h2>

<p>Pose Estimation(이하 ‘P.E’)은 컴퓨터 비전과 AI 분야에서 기본이 되는 task입니다. 대상 이미지나 영상 속에서 묘사되는 인체 부위가 갖는 방향성이나 위치를 감지(Detecting)하고 연결(Associating), 추적(Tracking)하는 것을 포함하고 있습니다.</p>

<p>감지, 연결, 추적의 대상은 Semantic Key point이며 인간의 pose를 estimation할 경우 대표적인 예로 “right shoulders”,”left knees”가 있습니다.</p>

<p>사물의 pose를 estimation할 경우(Object pose estimation) 대표적인 예로 자동차의 “left brake lights of vehicles”가 있습니다.
<img src="https://velog.velcdn.com/images/jinnij/post/10f69d18-0c37-473d-83ea-0c0ba0bf38a5/image.png" alt="Vehicle Pose Estimation using OpenPifPaf" /></p>

<p>라이브 영상에서 semantic keypoint를 추적하게 된다면, P.E의 정확도에 Limit을 걸어도 매우 큰 컴퓨팅 리소스를 필요로 하게 됩니다.</p>

<p>다만, Hardware와 모델 효율성이 최근에 크게 발전함에 따라 real-time 처리의 요구가 있는 새로운 application이 구현 및 경제적 실현이 가능해지고 있는 추세입니다.</p>

<p>최근 이미지 처리 분야에서 가장 powerful한 model은 다수가 CNN(Convolutional Neural Network)를 기반으로 하고 있습니다. 따라서 인간과 물체의 pose를 추론하는 application의 SOTA(State-of-the-Art) method는 CNN Architecture기반으로 이뤄집니다.</p>

<h2 id="bottom-up-vs-top-down-methods">Bottom-up Vs. Top-down methods</h2>

<p>P.E의 모든 접근법은 두 그룹으로 나뉠 수 있습니다.</p>

<ul>
  <li>
    <p>Bottom-up methods
우선 신체에 있는 모든 관절을 estimate한다음, 특정한 포즈 또는 하나의 사람 객체의 포즈로 그룹지어주는 방식이다.
“<strong>DeepCut</strong>모델”이 선도한 영역이라고 합니다.
이 방식의 문제점음 찾은 관절을 매칭할 수 있는 조합이 매우 많고(가령, 사람 1의 팔꿈치를 사람2에 붙일 수도 있겠다) 이를 적절하게 매칭하는데 시간이 많이 걸리며 정확도를 높이는 것이 힘듭니다..
하지만 사람을 먼저 감지하는 과정을 거치지 않기 때문에 Real-time에 적용이 가능합니다.</p>
  </li>
  <li>
    <p>Top-down methods
사람을 먼저 detecting한 후 detecting box 안에 있는 신체 관절을 estimate합니다. 문제점은 사람을 인식하지 못하면 자세 자체를 측정할 수 없고 사람의 수가 많아지면 계산량도 많아집니다.</p>
  </li>
</ul>

<h2 id="pose-estimation의-중요성">Pose Estimation의 중요성</h2>

<p>전통적인 object detection에서 인간은 오직 bounding box로만 인식이 되었습니다.</p>

<p>네모 박스는 그저 인간의 위치만 파악할 뿐 해당 인간이 어떠한 의도가 있는지, 자세는 어떤지, 어떠한 상황인지를 파악하기 어렵죠.</p>

<p>그렇기 때문에 Pose를 detection하고, tracking함으로서 컴퓨터는 인간의 body language를 더욱 잘 이해할 수 있게 되는 것 입니다.</p>

<p>그러나 기존의 pose tracking 방법은 Occlusion을 이겨낼 만큼 robust하지도, fast하지도 않은 한계가 존재했습니다.</p>

<p>그럼에도 최근에는 고성능의 real-time pose detection과 tracking이 비전 분야에서 메가 트렌드로 자리매김하고 있습니다. 예를 들어, 인간의 pose를 real-time으로 추적하면 컴퓨터가 인간 행동을 보다 세밀하고 자연스럽게 이해 할 수 있게 됩니다.
<img src="https://velog.velcdn.com/images/jinnij/post/98f23b79-f6de-4319-abf7-8dd58c8c9f96/image.png" alt="Crowd pose estimation with multi-instance analysis" /></p>

<p>자율 주행, 스포츠, 헬스케어 등등 많은 산업군에 영향을 미칠 수 있게 되는 것입니다. 오늘날 자율 주행 자동차의 사고(사실 진정한 의미의 자율주행은 아니죠. 산업계에서 자율주행차라고 지칭하는 것으로 이해하겠습니다.)의 대부분은 테슬라의 오토파일럿과 같이, 기술이 미성숙한 상태에서 인간이 완전히 기계를 믿기에 발생하는 건이죠. 
만약 인간의 pose를 detection하고 tracking할 수 있습니다면, 컴퓨터는 보행자나 in cabin의 행동을 더욱 이해하여 안전하고 natural한 driving을 할 수 있을 것입니다.
<img src="https://velog.velcdn.com/images/jinnij/post/fb617303-6d54-407e-8068-ab56291dc46b/image.png" alt="" /></p>

<h2 id="human-pose-estimation">Human Pose Estimation</h2>

<p>이제는 P.E의 구체적이 예시를 깊이 들여다 보겠습니다..
Human P.E는 이미지나 비디오 상에 나오는 신체나 관절의 pose를 예측하는 것을 주 목적으로 합니다. pose의 모션은 인간의 특정한 행동에 의해 발생되기 때문에 신체의 pose를 이해하는 것은 동작인식과 video 상의 인간의 동작을 이해하는 <a href="https://viso.ai/deep-learning/pytorchvideo-video-understanding/">Video understanding</a> 분야에서 매우 중요한 문제입니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/4530e87a-7183-4371-b4a8-6f6de56af880/image.png" alt="" /></p>

<h2 id="2d-human-pose-estimation이란">2D Human Pose Estimation이란?</h2>

<p>인체의 keypoint의 2D 위치 또는 공간 위치를 추정하는 것을 의미합니다. 전통적인 2D Human P.E 방법은 개별 신체 부위에 대해 각기 다른 수작업 특징 추출 기술을 사용하였습니다.</p>

<p>초기 컴퓨터 비전 작업은 global pose structure를 얻기 위해 인간의 신체를 막대기 모양으로 묘사했습니다. 그러나 최신 딥러닝 기반 접근 방식은 single-person과 multi-person의 P.E 분야에서 모두 성능을 크게 개선하였습니다.</p>

<p>대표적인 2D Human P.E 기법으로는 OpenPose, CPN, AlphaPose, HRNet이 있습니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/8d3ce8a4-966e-43cb-a34b-20a7b37c494c/image.gif" alt="" /></p>

<h2 id="3d-human-pose-estimation이란">3D Human Pose Estimation이란?</h2>

<p>3D 공간 상에서 신체 관절의 위치를 예측하는 것을 의미합니다. 게다가 3D 포즈 외에도 일부 방법은 이미지 또는 비디오에서 3D 인간 메쉬를 복구합니다. 이 분야는 인체의 광범위한 3차원 구조 정보를 얻는 데에 활용되기 때문에 최근 많은 관심을 받고 있습니다.</p>

<p>다양한 분야에 사용될 수 있는데, 3D 애니메이션 분야, VR/AR, 3D 행동 예측 분야 등이 대표적인 예이다. 3D Human P.E는 monocular 이미지 또는 비디오에서 수행될 수 있습니다.</p>

<p>multiple viewpoint 혹은 IMU, Lidar와 같은 센서를 사용하는 퓨전 기술을 적용한 3D P.E는 매우 어려운 작업이라고 합니다. 2D Human dataset은 얻기가 쉬운 반면, 3D는 상대적으로 어렵습니다. 3D pose의 정확한 image annotaion은 너무 많은 시간이 소요되며, 수동 레이블링(manual labeling)은 실용적이지도 않고 비용이 많이 듭니다.</p>

<p>그래서 비록 3D P.E이 최근 몇년 간 2D P.E의 진전으로 인하여 상당한 발전을 이뤘지만 극복해야할 몇 가지 산이 있습니다고 합니다.
대표적 예시는 아래와 같습니다.</p>
<ul>
  <li><strong>Model generalization</strong></li>
  <li><strong>robustness to occlusion</strong></li>
  <li><strong>computation efficiency.</strong></li>
</ul>

<p>실시간 3D Human P.E를 위해 Neural Network를 사용한 대표적인 라이브러리로는 OpenPose가 있습니다.</p>

<h2 id="3d-human-body-modeling">3D Human Body Modeling</h2>

<p>human P.E에서 인체 부위의 위치는 시각적 입력 데이터로부터 인체 표현(ex: skeleton pose)을 구축하는 데에 사용됩니다. 즉, 시각적 입력 데이터에서 추출한 특징 및 키포인트를 나타내는 데에 사용됩니다. 그렇기에 Human body Modeling이 human P.E에서 중요한 것입니다.</p>

<p>일반적으로 모델 기반의 접근 방식은 인체 pose를 설명 및 추론하고 2D 혹은 3D 포즈를 렌더링 하는 데에 사용됩니다.</p>

<p>대부분의 방법은 N-joint kinematic model을 사용합니다. 이 모델은 인체를 신체 운동학적 구조와 체형 정보를 포함하는 관절과 팔다리가 있는 개체로 표현합니다.</p>

<p>Body Modeling에는 3가지 타입이 있습니다.</p>

<h3 id="kinematic-model">Kinematic Model</h3>
<p>Skeleton-based model이라고도 불리며, 2D와 3D P.E에 사용됩니다. 직관적인 바디 모델로서, 관절의 위치나 팔다리(사지)의 방향과 같이 인체 구조를 나타내는 핵심 정보를 포함하고 있습니다.</p>

<p>이에, 서로 다른 신체 부위 간의 관계를 포착하는 데에 활용되기도 합니다. 하지만, 해당 모델은 Texture 혹은 shape에 관한 정보를 표현하는 데에 부족하다는 한계점이 있습니다.</p>

<h3 id="planar-model">Planar Model</h3>
<p>Contour-based model이라고도 불리며, 2D P.E에 사용됩니다. 이는 인체의 shape과 appearance를 나타내는 데에 사용됩니다. 주로, 인체의 외곽선을 따라 내부를 최대한 꽉꽉 채우는 작은 직사각형들로 인체의 부위를 표현합니다.</p>

<p>대표적인 모델로 Active Shape Model(ASM)이 있으며, Principle component analysis(주성분 분석)을 이용하여 전체 인체 그래프와 실루엣 변형을 포착하는 데에 활용됩니다. 이는 60,000개 이상의 고해상도의 전신 스캔 데이터셋에서 훈련된 완전히 훈련 가능한 end-to-end 딥러닝 파이프라인입니다. 이를 통해 통계적이며, 관절로 분절된 3D 인체 shape과 pose을 모델링 할 수 있습니다.</p>

<h3 id="volumetric-model">Volumetric Model</h3>

<p>3D P.E에 사용됩니다. 3D human mesh 복구를 위한 딥러닝 기반의 3D human P.E에 사용 되는 몇몇 인기 있는 3D 신체 모델이 있습니다. 대표적인 예로 GHUM &amp; GHUML(ite)이 있습니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/937187fc-dc22-407d-9529-f18a23dcf277/image.png" alt="" /></p>

<h2 id="pose-detection의-주요-챌린지">Pose Detection의 주요 챌린지</h2>

<p>Human P.E은 신체의 외형이 계속해서 변해가기 때문에 매우 어려운 Task중 하나입니다. 옷의 형태, randomic하게 occlusion되는 경우, 시야각, 배경의 문맥(상황이라고 이해해본다) 등 많은 요소로 인해 변화가 일어납니다. 나아가 P.E는 빛이나 날씨와 같은 리얼 세계의 수많은 변화에 대응할 수 있을만큼 robust해져야 한다는 도전 과제가 쌓여 있습니다.</p>

<h2 id="head-pose-estimation">Head Pose Estimation</h2>

<p>인간 머리의 pose를 estimate하는 것은 유명한 computer vision 문제입니다. 여기에는 여러 application이 활용되는데 aiding in gaze estimation, modeling attention, fitting 3D models to video, performing face alignment 등이 대표적입니다.</p>

<p>전형적으로 머리 pose를 찾을 때는 대상 얼굴의 keypoint를 사용하고 2D에서 3D pose로의 대응 문제는 mean human head model로 해결합니다.</p>

<p>머리의 3D pose 리커버 능력은 딥러닝 기법을 활용한 2D facial keypoint 추출에 기반한 keypoint-based 표정 분석으로 부터 나오게 됩니다. 해당 방식을 통해 occlusion과 다양한 포즈 변화에 강건한 형태가 되었습니다.</p>

<h2 id="animal-pose-estimation">Animal Pose Estimation</h2>

<p>대부분 SOTA를 달성한 기법들은 인체 포즈에만 관심이 있지만, 몇몇 모델은 동물과 자동차(사물)을 Estimation하기 위해 개발되어 왔습니다.</p>

<p>Human E.P와 달리 추가적인 어려움이 존재하는데, 제한된 라벨 데이터(데이터를 수집하고 수동으로 이미지에 주석을 추가해야하는 등)와 너무도 많은 self-occlusion이 문제입니다. 그래서 동물 포즈 추정을 위한 데이터셋은 아주 제한된 숫자의 동물 종을 포함하고 있습니다.</p>

<p>이렇게 사용 가능한 데이터가 한정적이고 작은 데이터셋으로 작업을 수행할 때는 Active learning과 data augmentation 기법이 필요로 하게 됩니다. 양 기술은 vision 알고리즘을 더욱 효과적으로 학습시키고 맞춤형 AI model 학습을 위한 annotation 작업량을 감소시켜 줍니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/2361edcb-789c-4291-9a87-8a7d93d8df7b/image.png" alt="" /></p>

<p>또한, 다수의 동물의 포즈를 추정하는 것은 어려운 computer vision 문제입니다. 동물간 빈번한 상호작용으로 인해 occlusion을 야기하고 감지된 keypoint를 올바르게 객체마다 할당하는 것을 복잡하게 만들기 때문입니다. 나아가 인간이 볼 때, 매우 비슷하게 생긴 동물이 통상 인간 세계의 상호 작용보다 더욱 긴밀하게 상호작용한다면 이때의 다수 동물의 포즈를 추정하는 것도 어렵겠습니다.</p>

<p>이러한 문제를 해결하기 위해 인간에서 동물로 방법을 다시 적용하는 transfer learning 기술이 개발 되었습니다. 대표적인 예시로 다수의 동물의 포즈를 추정하고 추적할 때 DeepLabCut을 사용합니다. 이는 인간과 동물의 포즈를 추정하는 오픈 소스 툴 박스이며 SOTA를 찍는 모델입니다.</p>

<p>동물 포즈 추적에 관련한 컴퓨터 비전 기술의 응용은 아래 글을 참고하면 됩니다.
<a href="https://viso.ai/applications/computer-vision-in-agriculture/">Computer vision in agriculture</a></p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/4404a68c-5472-4485-bd63-ea9f32693206/image.gif" alt="" /></p>

<h2 id="video-person-pose-tracking">Video Person Pose Tracking</h2>

<p>복잡한 상황에서의 Multi-frame human pose estimation은 매우 복잡하고 높은 컴퓨팅 파워를 요합니다. 인간의 관절 detector가 정적 이미지에서는 좋은 성능을 발휘하는 반면에, ML 모델이 실시간 포즈 추적을 위해 비디오 시퀀스에 적용될 때는 성능이 종종 부족해집니다.</p>

<p>가장 큰 문제들 중 일부는 motion blur 처리, video defocus, pose occlusion, 비디오 프레임 간의 시간적 종속성을 포착할 수 없는 등의 어려움이 있습니다.</p>

<p>기존의 RNN을 적용하면, 특히 pose occlusion을 처리하는데에 공간의 컨텍스트를 모델링하는 데에 경험적인 어려움이 발생하게 됩니다. multi-frame에서 인간의 자세를 추정하는 SOTA를 찍는 프레임워크인 “DCPose”는 비디오 프레임 사이의 풍부한 temporal 단서를 활용하여 eypoint detection을 용이하게 합니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/8b2c95f7-ceeb-48fb-84fe-2be7b166ce9a/image.gif" alt="" /></p>

<h2 id="pose-estimation은-어떻게-동작할까">Pose Estimation은 어떻게 동작할까?</h2>

<p>대부분의 포즈 추정기(pose estimator)는 2단계의 프레임워크로 구성되어 있습니다. 인간의 bounding box를 감지한 다음 각 box내에서 포즈를 추정합니다.</p>

<p>포즈 추정은 사람이나 사물의 keypoint를 찾는 방식으로 작동합니다. 예를 들어 사람의 팔꿈치, 무릎, 손목 등과 같은 관절이 핵심 포인트가 됩니다.</p>

<p>보편적인 MS COCO 데이터셋에서의 인간 자세 추정은 17개의 서로 다른 keypoint(class)를 감지할 수 있습니다. 각 키포인트에는 세개의 숫자(x,y,v)가 annotated됩니다. x,y는 target point의 좌표이고, v는 키포인트인지 아닌지 여부를 나타냅니다.
(key : v=1, non-key : v=0)</p>

<p>“nose”, “left_eye”, “right_eye”, “left_ear”, “right_ear”, “left_shoulder”, “right_shoulder”, “left_elbow”, “right_elbow”, “left_wrist”, “right_wrist”, “left_hip”, “right_hip”, “left_knee”, “right_knee”, “left_ankle”, “right_ankle”</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/dffae4c9-d171-4b71-82ae-bbfe5b74a6ee/image.png" alt="" /></p>

<h3 id="딥러닝-기반의-pose-esimation">딥러닝 기반의 Pose Esimation</h3>

<p>딥러닝의 최근 급격한 발전으로 인해 기존 컴퓨터 비전 분야에서 image segmentation / object detection 분야의 좋은 성과를 내왔다. 그 결과 P.E 분야에도 좋은 성과를 낼 수 있었습니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/6a5939d5-7c39-49f2-ae10-3cd3f174c2cf/image.png" alt="" /></p>

<h3 id="가장-유명한-pose-estimation-기법">가장 유명한 Pose Estimation 기법</h3>

<ul>
  <li>Method #1: OpenPose</li>
  <li>Method #2: High-Resolution Net (HRNet)</li>
  <li>Method #3: DeepCut</li>
  <li>Method #4: Regional Multi-Person Pose Estimation (AlphaPose)</li>
  <li>Method #5: Deep Pose</li>
  <li>Method #6: PoseNet</li>
  <li>Method #7: Dense Pose</li>
</ul>

<h3 id="상기-기법들의-설명">상기 기법들의 설명</h3>

<p>포즈 추정은 비교적 쉽게 적용해볼 수 있는 비전 기술이기 때문에, 기존 아키텍처를 사용하여 맞춤형 포즈 추정기를 구현해볼 수 있습니다. 이를 위한 기존 아키텍쳐는 아래와 같습니다.</p>

<ol>
  <li>
    <p>OpenPose
가장 유명한 bottom-up 방식의 multi-person human pose estimation 기법입니다. 신체, 발, 손 및 얼굴 키포인트를 높은 정확도로 감지하는 오픈 소스 실시간 다중 사람 감지가 가능합니다. OpenPose API의 장점은 사용자가 카메라 필드, 웹캠 등에서 소스 이미지를 선택할 수 있는 유연성을 제공한다는 점입니다.(예를 들어 CCTV 카메라와 시스템과의 통합) CUDA GPU, OpenCL GPU 또는 GPU 전용 장치와 같은 다양한 하드웨어 아키텍쳐를 지원합니다. 경량 버전은 Edge device에서 실시간으로 온디바이스 프로세싱이 가능한 edge 추론 어플리케이션으로 충분히 사용 가능합니다.</p>
  </li>
  <li>
    <p>High-Resolution Net(HRNet)
인간 포즈 추정을 위한 neural network이다. 이미지에서 특정 개체 또는 사람과 관련하여 키 포인트(관절)로 알고 있는 것을 찾기 위해 이미지 처리애 사용되는 아키텍쳐입니다. 다른 아키텍처에 비해 이 아키텍처의 장점은 대부분의 기존 방법이 고해상도 네트워크 사용과 관련하여 저해상도 표현에서 자세의 고해상도 표현과 일치한다는 점입니다. 이 bias 대신, 신경망은 자세를 추정할 때 고해상도 표현을 유지합니다. 예를 들어, TV 스포츠에서 사람의 자세를 감지하는 데에 도움이 됩니다.</p>
  </li>
  <li>
    <p>DeepCut
또 다른 bottom-up 방식의 multi-person human P.E기법입니다. 이미지 상에서 사람의 수를 감지한 다음, 각 이미지마다의 관절의 위치를 예측합니다. DeepCut은 가령 비디오 상에서의 농구, 축구 등 스포츠나 기타 상황에서 인간과 물체를 estimating하는 데에 적용할 수 있습니다.</p>
  </li>
  <li>
    <p>Regional Multi-Person Pose Estimation(AlphaPose)
유명한 top-down 방식의 P.E이다. 부정확한 인간 Bounding box가 있을 때 포즈를 감지하는 데에 유용합니다. 즉 최적으로 검출된 바운딩 박스를 통해 인간 포즈를 추정하기 위한 최적의 아키텍쳐입니다. 이미지나 영상에서 싱글 혹은 다수의 사람의 포즈를 검출하는 데에 사용될 수 있습니다.</p>
  </li>
  <li>
    <p>DeepPose
Deep Neural network를 활용한 human P.E이다. 모든 관절을 포착하고 pooling layer, convolution layer, fully-connected layer를 연결하여 계층의 일부를 형성합니다.</p>
  </li>
  <li>
    <p>PoseNet
브라우저나 모바일 장치와 같은 가벼운 장치에서 실행하기 위해 tensorflow.js에 구축된 포즈 추정기 아키텍처입니다. 따라서 PoseNet를 사용하여 단일 혹은 여러 포즈를 추정할 수 있습니다.</p>
  </li>
  <li>
    <p>DensePose
RGB이미지의 모든 인간 픽셀을 인체의 3D 표면에 매핑하는 것을 목표로하는 P.E 기법입니다. 단일/다수의 P.E에 적용할 수 있습니다.</p>
  </li>
  <li>
    <p>TensorFlow Pose Estimation</p>
  </li>
  <li>
    <p>OpenPifPaf
P.E를 위한 오픈 소스 형 컴퓨터 비전 라이브러리 및 프레임워크입니다. 이미지 또는 비디오에서 인체 부위를 식별하고 위치를 파악하는 것과 관련이 있습니다. PyTorch 딥 러닝 프레임워크 위에 구축되었으며 multi-task learning 접근 방식을 사용하여 정확하고 효율적인 포즈 추정을 합니다. 사용성이 좋으며 강건성, 어려운 P.E 시나리오(가령, occlusion, cluttered background)를 다룰 수 있기 때문에 유명해졌습니다.
<img src="https://velog.velcdn.com/images/jinnij/post/72ac558c-1449-4181-bfbc-c7f6a8423b20/image.png" alt="" /></p>
  </li>
</ol>

<h2 id="use-cases--applications-of-pose-estimation">Use Cases &amp; Applications of Pose Estimation</h2>

<h3 id="가장-유명한-pose-estimation-applications">가장 유명한 Pose Estimation applications</h3>

<p>다양한 분야에서 사용됩니다 가령. <strong>Human-computer interaction(HCI), Action Recognition, Motion Capture, Movement analysis, Augmented reality, Sports and Fitness, Robotics.</strong></p>

<ul>
  <li>Application #1: Human Activity Estimation</li>
  <li>Application #2: Motion Transfer and Augmented Reality</li>
  <li>Application #3: Motion Capture for Training Robots</li>
  <li>Application #4: Motion Tracking for Consoles</li>
  <li>Application #5: Human Fall Detection
<img src="https://velog.velcdn.com/images/jinnij/post/e147972f-2e55-4bd0-bb4b-9d7d8059966b/image.png" alt="" /></li>
</ul>

<h3 id="상기-applications의-설명">상기 applications의 설명</h3>

<ol>
  <li>Human Activity Estimation
확실한 사용처는 인간의 활동과 움직임을 추적하고 측정하는 것입니다. DensePose, PoseNet, OpenPose는 종종 활동, 제스처 또는 보행 인식에 사용됩니다. 자세 추정을 통한 인간 활동 추적의 예는 다음과 같습니다.</li>
</ol>

<ul>
  <li>
    <p>앉은 제스처 감지, 손 제스처 인식 또는 얼굴 표정 분석을 위한 애플리케이션</p>
  </li>
  <li>운동 선수에 대한 AI 기반 분석</li>
  <li>댄스 기술 분석을 위한 애플리케이션(예: 발레)</li>
  <li>의료 수술 서비스의 품질을 평가하기 위한 컴퓨터 비전 시스템</li>
  <li>운동의 실행 형태를 감지학 반복 횟수를 카운트하는 피트니스 어플리케이션</li>
  <li>전신/ 수화 커뮤니케이션(예 : 교통 경찰 신호)</li>
  <li>넘어지는 사람의 감지 혹은 특정 질병의 진행과정을 감지하는 지능형 어플리케이션
<img src="https://velog.velcdn.com/images/jinnij/post/83ea2af8-da41-431e-ba73-1af9306c350f/image.png" alt="" /></li>
</ul>

<ol>
  <li>Augmented Reality and Virtual Reality
현재 AR/VR에 반영된 P.E기술은 사용자들에게 더 나은 온라인 경험을 제공하고 있습니다. 예를 들어 사용자는 포즈를 취하는 가상 튜터를 통해 테니스를 배우는 사례가 있습니다.</li>
</ol>

<p>또한 포즈 추정기는 증강 현실 기반 응용프로그램과 인터페이스할 수도 있습니다. 예를 들어 미 육군은 전투에 사용할 증강 현실 프로그램을 실험하기도 합니다. 이 프로그램은 병사들이 피아식별을 하고 야간 시력을 향상시키는 것을 목표로 한다고 합니다.
<img src="https://velog.velcdn.com/images/jinnij/post/bb71aae4-0fbb-4a3c-95cf-b0b50f25a9a6/image.png" alt="" /></p>

<ol>
  <li>Training Robots with Human Pose Tracking</li>
</ol>

<p>포즈 추정기의 일반적인 사용 사례는 로봇이 특정 기술을 배우도록 하는 어플리케이션에 있습니다. 궤적을 따르도록 수동으로 로봇을 프로그래밍하는 대신 로봇이 교사의 자세, 외모나 외형을 따라하여 행동을 배우도록 할 수 있습니다.</p>

<ol>
  <li>Human Motion Tracking for Consoles
게임 내 응용 프로그램으로도 사용할 수 있습니다. 인간이 대화형 게임 경험을 위해 게임 환경에 포즈를 자동 생성하고 주입하게 됩니다. 가령, Microsoft는 3D 포즈 추정(IR Sensor사용)을 사용하여 인간 플레이어의 동작을 추정하고 이를 사용하여 캐릭터의 동작을 게임 환경에 가상으로 렌더링했습니다.</li>
</ol>

<h1 id="reference">Reference</h1>

<p>https://ctkim.tistory.com/101<br />
viso.ai<br /></p>]]></content><author><name></name></author><category term="study" /><category term="computer_vision" /><category term="Human_Pose_Estimation" /><summary type="html"><![CDATA[recording_a Summary of a HPE]]></summary></entry><entry><title type="html">Computer Vision_Optical Flow</title><link href="https://chojinie.github.io/blog/2023/computer-vision-optical-flow-%EB%B3%B5%EC%82%AC%EB%B3%B8/" rel="alternate" type="text/html" title="Computer Vision_Optical Flow" /><published>2023-05-22T00:00:00+00:00</published><updated>2023-05-22T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/computer-vision-optical-flow%20-%20%EB%B3%B5%EC%82%AC%EB%B3%B8</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/computer-vision-optical-flow-%EB%B3%B5%EC%82%AC%EB%B3%B8/"><![CDATA[<p>석사 과정 재학 중, 공부한 내용을 기록하기 위해 작성하는 글입니다.</p>

<h2 id="optical-flow의-정의">Optical Flow의 정의</h2>

<p>옵티컬 플로우는 관찰자와 장면 간의 상대적인 움직임으로 인해 발생하는 물체, 표면 및 가장자리의 두드러지는 움직임 패턴을 의미합니다. 제가 이해한대로 표현하자면 영상(Image)이나 동영상에서 사물이나 배경의 움직임을 표현하는 기법인데, 사물이나 광원이 움직일 경우 인간의 눈에는 어떠한 흐름의 크기와 방향성이 느껴집니다. 이를 수식적으로 표현하는 것을 뜻합니다.</p>

<h2 id="motion-field와-optical-flow">Motion Field와 Optical Flow</h2>

<h3 id="motion-field">Motion Field</h3>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>\(\mathrm{v}_i\)(Image velocity) 와 \(\mathrm{v}_o\)(Seen velocity) 간의 관계를 나타내는 것이 초기 목적입니다. 
\(\Rightarrow\) Perspective projection을 이용하게 됩니다.</p>

<p>\(\mathrm{v}_i\) 나 Motion Field를 측정할 수 없으므로 Brightness Pattern을 측정할 수 밖에 없습니다.
연속되는 두 이미지 간에 포인트의 모션은 두 포인트의 Depth와 관련이 있습니다.
오른쪽 그림에서 Pinhole은 Z 평면을 가지고 있습니다. 삼각형 모양 간의 비례식으로 표현하여 식을 구할 수 있게 됩니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of3.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>위에서 Motion Field는 측정이 어려우며, Brightness Pattern을 측정할 수 밖에 없다고 했습니다. 즉, 계산 과정에서 물체의 움직임을 명시적으로 반영하지는 않지만
물체가 움직이면 그에 따른 명암 변화가 발생하므로 물체의 움직임인 Motion Field를 반영한다고도 할 수 있습니다.
오른쪽 그림의 벡터의 길이(크기)는 시간 내에 얼마나 빨리 움직였는지를 나타내며, 화살표의 방향은 어느 방향으로 이동하는지를 나타내줍니다. 
이상적으로는 optical flow는 motion field와 같게 표시 될 수 있습니다. 하지만 현실 세계에서 많은 경우 그 둘은 같지 않게 됩니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of4.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>위 두 그림의 공통점은 광원(Source)과 구체(Sphere)가 있다는 것입니다. 구체는 Lambertian BRDF라고 생각하면 좋을 것 같습니다. 이에 대해서는 추후 보완 설명하겠습니다.
왼쪽의 그림은 광원의 위치는 그대로 두고, 구체가 회전하는 상태입니다. 즉, Motion Field는 존재하지만 Optical Flow는 발생하지 않게 됩니다.
오른쪽 그림은 이와 반대입니다. 구체는 가만히 있지만 광원이 움직여서 밝아 보이는 위치에 변화가 발생합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of5.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h3 id="optical-flow">Optical Flow</h3>

<p>실생활의 예시를 들어보겠습니다. 이발소 영업 상태를 나타내는 돌돌이가 있죠. 실제로 그 안의 패턴은 오른쪽으로 움직여서 Motion Field는 왼쪽에서 오른쪽을 향하게 됩니다.
하지만, 우리가 눈으로 볼 때는 위에서 아래로 움직이는 것 같은 효과가 발생하죠. 즉 Motion Field와 Optical Field가 해당 경우에는 orthogonal한 관계임을 보여줍니다.
아래 두 그림 역시 움직임은 없으나 시각적으로 흐르고 있는 것을 느낄 수 있습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of6-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of6-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of6-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of6.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>그렇다면 이제는 Optical Flow에 대해 얘기해보겠습니다. 사진 상의 새가 \(\delta t\) 만큼의 시간동안 이동하였습니다. 즉 변위(Displacement)가 발생한 것입니다.
그 찰나동안 x, y 방면으로 이동하는 동안의 속도(u, v)를 point와 연관이 있는 Optical Flow라고 표현합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>Optical Flow가 성립되기 위해서는 중요한 가정들이 몇가지 있습니다. 이를 Optical Flow Constraint Equation이라고 표현하며, 비단 Optical Flow 뿐 아니라 
Computer Vision분야에서는 이러한 ‘제약’들을 걸어서 많은 문제를 푸는 것을 알 수 있습니다.</p>

<p>우선 모든 시간에 걸쳐서 이미지의 Point들의 brightness는 불변하다고 전제합니다. 매우 짧은 순간인 \(\delta t\) 만큼의 시간이기에 가능한 가정인 것 같습니다.
위와 같이 수식으로도 표현할 수 있죠.(I = Intensity = Brightness)</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of8-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of8-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of8-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of8.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>두번째로는 \((\delta x, \delta y, \delta t)\)는 매우매우 작아야 한다는 것입니다. 이를 통해 Taylor Expansion을 적용하여 수식을 간소화 할 수 있습니다.
아래 Taylor Series Expansion의 설명을 참고하시면 됩니다.</p>

<h4 id="taylor-series">Taylor Series</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of9-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of9-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of9-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of10.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of11.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>가정들을 종합하면 위와 같이 식을 나타낼 수 있게 됩니다. Constraint Equation으로 \(\mathbf{I}_x{u} + \mathbf{I}_x{v} + \mathbf{I}_t = 0\) 가 도출됩니다.
또한 \(\mathbf{I}_x, \mathbf{I}_y, \mathbf{I}_t\)는 이미지의 두 프레임만으로 쉽게 구할 수 있습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of12.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>Constraint를 optical flow 좌표상에 표현해보면 아래와 같이 나타낼 수 있습니다. 좌표로 나타낸다면, 성분을 분리하여 나타낼 수도 있죠.
Normal Flow의 방향, 크기를 각각 식으로 구분할 수 있습니다. 하지만 constraint line과 평행한 \(u_p\) 는 무한하므로 특정지을 수가 없습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of13.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>이러한 ambiguity는 Aperture Problem을 야기하게 됩니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of14.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>line의 실제 움직임은 오른쪽 아래 방향으로 이루어집니다. 하지만, local(일부분)만 볼 경우 오른쪽 위 방향으로 움직이는 것처럼 보이게 됩니다.
이를 Aperture Problem이라고 하며 이를 해결하기 위한 해결책은 후술하겠습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of15.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>결과적으로 Under constraint한 환경에서 optical flow를 구하게 되는 것이고, 이를 찾기 위한 몇가지 알고리즘을 이제 소개하도록 하겠습니다.</p>

<h2 id="lucas-kanade-optical-flow">Lucas-Kanade Optical Flow</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of16.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>Lucas-Kanade 방식은 local 방식입니다. 픽셀 \((x, y)\) 를 중심으로 하는 윈도우 영역 \(\mathbf{W}\)의 Optical Flow는 같다고 가정합니다.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of17.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>아래와 같이 모든 point를 쌓아서 matrix form으로 나타낼 수 있게 되고 계산이 편리해 집니다.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of18.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of19-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of19-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of19-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of19.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>그렇다면, 이러한 조건을 갖기 위해서는 어떻게 해야할까요?<br />
\(\bullet \mathbf{A}^T\mathbf{A}\) 는 반드시 invertible해야 합니다. determinant \(\neq 0\) 이어야만 계산할 수 있기 때문이죠.<br />
\(\bullet \mathbf{A}^T\mathbf{A}\) 는 Well-conditioned 상태여야 합니다. 풀어쓰자면,<br />
\(\mathbf{A}^T\mathbf{A}\) 의 eigen value인 \(\lambda_1 , \lambda_2\) 는 positive definite해야하며,<br /> 
eigen value를 나열하는 순서대로 \(\lambda_1 \geq \lambda_2\)이지만, 그 크기의 차이가 너무 커서도 안됩니다.<br /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of20.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>해당 조건에 대해 자세히 살펴보겠습니다.
만약에 eigen value가 너무 작고 크기에 차이가 거의 없는 경우 \(\mathbf{A}^T\mathbf{A}\)는 well condition이 아니게 됩니다.
반면 오른쪽 그림처럼 Edge라서 eigen value값의 차이가 너무 큰 경우, Aperture problem과 같은 side effect가 발생합니다.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of21-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of21-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of21-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of21.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of22-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of22-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of22-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of22.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>아래와 같이 texture가 있는 영역은 optical flow 계산에 좋은 영역입니다.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of23-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of23-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of23-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of23.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>그렇다면, 만약 사물의 위치 변화가 크면 어떻게 될까요? 이 경우에는 \(\delta t\) \(\delta x\) \(\delta y\)를 가정하기 어렵습니다.
그래서 constraint를 가하기도 어렵구요. 결국 Aliasing이 일어나게 됩니다.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of24-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of24-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of24-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of24.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>이를 해결하기 위한 방법으로 Coarse-to-Fine Estimation방법이 있습니다. 직역하자면 거친 이미지에서 좋은 이미지로 옮겨가며 추정한다는 것입니다.
거친 이미지는 저해상도를 좋은 이미지는 고해상도의 이미지를 의미합니다. 이러한 워딩이 왜 나왔는지는 아래 그림으로 설명이 가능합니다.
왼쪽의 원본 이미지 상에서는 \(\delta t\)의 시간 동안 사물이 꽤 많은 위치를 이동한 것으로 보입니다. 이러한 Gap을 줄이려면 downsampling을 하면
가능합니다. 맨 우측의 N/8 사이즈의 저해상도 이미지의 경우 1pixel안에서 모션의 gap 차이를 담을 수 있게 됩니다. 이 상황에서는 optical flow의
contraint equation을 다시 적용할 수 있게 되는 것입니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of25-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of25-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of25-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of25.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of26-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of26-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of26-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of26.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>아래와 같이 optical flow가 잘 나타나는 것을 볼 수 있습니다.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of27-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of27-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of27-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of27.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h2 id="horn-schunck-optical-flow">Horn-Schunck Optical Flow</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of28-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of28-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of28-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of28.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>LK 방식은 지역적 방법이므로 굉장히 Sparse한 결과를 낸다는 단점이 있었습니다. LK의 윈도우 내부만을 보는 방식이 아니라 이미지 전역(GLOBAL)에서 
이를 극복하기 위해서는 Smoothness를 더하는 방안이 있겠는데요, 보통 사물은 단단하거나 탄력적으로 변형되며, 포인트가 개별이 아닌 사물 전체가 함께 움직인다는 점에서 착안된 방식입니다.
Horn-Schunck 방식의 key idea는 Frame간의 움직임이 작아서, brightness의 일관성과 Optical Flow의 균일성을 강제로 둔다는 것입니다.</p>

<h3 id="enforce-brightness-constancy">Enforce Brightness constancy</h3>

<p>Brightness를 일관되게 유지하기 위해서는 \(\mathbf{I}_x{u} + \mathbf{I}_y{v} + \mathbf{I}_t = 0\)를 다시 불러옵니다.
모든 픽셀에 대해 적용해야 하기 때문에 아래 식을 만족하면 되겠네요.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of29-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of29-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of29-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of29.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h3 id="enforce-smooth-flow-field">Enforce Smooth flow field</h3>

<p>smoothness는 아래처럼 표현하면 간단할 것 같습니다.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of30-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of30-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of30-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of30.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>좀 더 쉽게 설명해보겠습니다. smooth하다면 optical flow 맵은 균일할 것입니다. 균일한 정도는 아래 식을 사용하여 추정합니다.
gradient가 작다면 이웃한 픽셀간의 u, v가 비슷하다는 의미이므로 optical flow가 균일하다고도 볼 수 있습니다. 아래 이미지처럼 오른쪽은 균일하지 못하죠.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of32-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of32-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of32-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of32.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of33-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of33-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of33-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of33.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>위 두 key idea를 합치면 optical flow를 계산할 수 있습니다. 결국, smoothness와 brightness constancy를 최소화 시키는 방향의 해를 구해야 합니다.
여기서 \(\lambda\)는 어느 것에 더 비중을 둬야하는지를 나타내는 가중치를 의미합니다. 이와 같이 가중치가 붙어 있는 항을 regularization term이라고 하며,
균일한 optical flow 해를 구하는 방법을 정규화 기법이라고 합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optical_flow_study/of31-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optical_flow_study/of31-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optical_flow_study/of31-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/optical_flow_study/of31.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h2 id="reference">Reference</h2>

<p>Optical flow. (2023, April 29). In Wikipedia. https://en.wikipedia.org/wiki/Optical_flow<br /></p>

<p>https://youtube.com/@firstprinciplesofcomputerv3258<br /></p>

<p>https://gaussian37.github.io/vision-concept-optical_flow/<br /></p>

<p>http://www.cs.cmu.edu/~16385/<br /></p>]]></content><author><name></name></author><category term="study" /><category term="optical_flow" /><category term="computer_vision" /><category term="Lucas_Kanade" /><category term="Horn_schunck" /><summary type="html"><![CDATA[recording_a Summary of a optical flow]]></summary></entry><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://chojinie.github.io/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar" /><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://chojinie.github.io/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post as a sidebar, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2>

<p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><category term="sidebar" /><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">displaying beatiful tables with Bootstrap Tables</title><link href="https://chojinie.github.io/blog/2023/tables/" rel="alternate" type="text/html" title="displaying beatiful tables with Bootstrap Tables" /><published>2023-03-20T18:37:00+00:00</published><updated>2023-03-20T18:37:00+00:00</updated><id>https://chojinie.github.io/blog/2023/tables</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/tables/"><![CDATA[<p>Using markdown to display tables is easy. Just use the following syntax:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| Left aligned | Center aligned | Right aligned |
| :----------- | :------------: | ------------: |
| Left 1       | center 1       | right 1       |
| Left 2       | center 2       | right 2       |
| Left 3       | center 3       | right 3       |
</code></pre></div></div>

<p>That will generate:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Left aligned</th>
      <th style="text-align: center">Center aligned</th>
      <th style="text-align: right">Right aligned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Left 1</td>
      <td style="text-align: center">center 1</td>
      <td style="text-align: right">right 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 2</td>
      <td style="text-align: center">center 2</td>
      <td style="text-align: right">right 2</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 3</td>
      <td style="text-align: center">center 3</td>
      <td style="text-align: right">right 3</td>
    </tr>
  </tbody>
</table>

<p></p>

<p>It is also possible to use HTML to display tables. For example, the following HTML code will display a table with <a href="https://bootstrap-table.com/">Bootstrap Table</a>, loaded from a JSON file:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">id=</span><span class="s">"table"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div>

<table data-toggle="table" data-url="/assets/json/table_data.json">
  <thead>
    <tr>
      <th data-field="id">ID</th>
      <th data-field="name">Item Name</th>
      <th data-field="price">Item Price</th>
    </tr>
  </thead>
</table>

<p></p>

<p>By using <a href="https://bootstrap-table.com/">Bootstrap Table</a> it is possible to create pretty complex tables, with pagination, search, and more. For example, the following HTML code will display a table, loaded from a JSON file, with pagination, search, checkboxes, and header/content alignment. For more information, check the <a href="https://examples.bootstrap-table.com/index.html">documentation</a>.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">data-click-to-select=</span><span class="s">"true"</span>
  <span class="na">data-height=</span><span class="s">"460"</span>
  <span class="na">data-pagination=</span><span class="s">"true"</span>
  <span class="na">data-search=</span><span class="s">"true"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-checkbox=</span><span class="s">"true"</span><span class="nt">&gt;&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span> <span class="na">data-halign=</span><span class="s">"left"</span> <span class="na">data-align=</span><span class="s">"center"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span> <span class="na">data-halign=</span><span class="s">"center"</span> <span class="na">data-align=</span><span class="s">"right"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span> <span class="na">data-halign=</span><span class="s">"right"</span> <span class="na">data-align=</span><span class="s">"left"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div>

<table data-click-to-select="true" data-height="460" data-pagination="true" data-search="true" data-toggle="table" data-url="/assets/json/table_data.json">
  <thead>
    <tr>
      <th data-checkbox="true"></th>
      <th data-field="id" data-halign="left" data-align="center" data-sortable="true">ID</th>
      <th data-field="name" data-halign="center" data-align="right" data-sortable="true">Item Name</th>
      <th data-field="price" data-halign="right" data-align="left" data-sortable="true">Item Price</th>
    </tr>
  </thead>
</table>]]></content><author><name></name></author><category term="sample-posts" /><summary type="html"><![CDATA[an example of how to use Bootstrap Tables]]></summary></entry><entry><title type="html">a post with table of contents</title><link href="https://chojinie.github.io/blog/2023/table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents" /><published>2023-03-20T15:59:00+00:00</published><updated>2023-03-20T15:59:00+00:00</updated><id>https://chojinie.github.io/blog/2023/table-of-contents</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents in the beginning of the post.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">beginning</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 id="table-of-contents-options">Table of Contents Options</h2>

<p>If you want to learn more about how to customize the table of contents, you can check the <a href="https://github.com/toshimaru/jekyll-toc">jekyll-toc</a> repository.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><summary type="html"><![CDATA[an example of a blog post with table of contents]]></summary></entry><entry><title type="html">a post with giscus comments</title><link href="https://chojinie.github.io/blog/2022/giscus-comments/" rel="alternate" type="text/html" title="a post with giscus comments" /><published>2022-12-10T15:59:00+00:00</published><updated>2022-12-10T15:59:00+00:00</updated><id>https://chojinie.github.io/blog/2022/giscus-comments</id><content type="html" xml:base="https://chojinie.github.io/blog/2022/giscus-comments/"><![CDATA[<p>This post shows how to add GISCUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with giscus comments]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://chojinie.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog" /><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://chojinie.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://chojinie.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a post with redirect</title><link href="https://chojinie.github.io/blog/2022/redirect/" rel="alternate" type="text/html" title="a post with redirect" /><published>2022-02-01T17:39:00+00:00</published><updated>2022-02-01T17:39:00+00:00</updated><id>https://chojinie.github.io/blog/2022/redirect</id><content type="html" xml:base="https://chojinie.github.io/blog/2022/redirect/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[you can also redirect to assets like pdf]]></summary></entry><entry><title type="html">a post with diagrams</title><link href="https://chojinie.github.io/blog/2021/diagrams/" rel="alternate" type="text/html" title="a post with diagrams" /><published>2021-07-04T17:39:00+00:00</published><updated>2021-07-04T17:39:00+00:00</updated><id>https://chojinie.github.io/blog/2021/diagrams</id><content type="html" xml:base="https://chojinie.github.io/blog/2021/diagrams/"><![CDATA[<p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin.
Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p>

<p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine.
Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW.
For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p>

<h2 id="mermaid">Mermaid</h2>

<p>Install mermaid using <code class="language-plaintext highlighter-rouge">node.js</code> package manager <code class="language-plaintext highlighter-rouge">npm</code> by running the following command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>npm <span class="nb">install</span> <span class="nt">-g</span> mermaid.cli
</code></pre></div></div>

<p>The diagram below was generated by the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div>

<div class="jekyll-diagrams diagrams mermaid">
  <svg id="mermaid-1684818902147" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1684818902147 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1684818902147 .node circle,#mermaid-1684818902147 .node ellipse,#mermaid-1684818902147 .node polygon,#mermaid-1684818902147 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1684818902147 .node.clickable{cursor:pointer}#mermaid-1684818902147 .arrowheadPath{fill:#333}#mermaid-1684818902147 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1684818902147 .edgeLabel{background-color:#e8e8e8}#mermaid-1684818902147 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1684818902147 .cluster text{fill:#333}#mermaid-1684818902147 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1684818902147 .actor{stroke:#ccf;fill:#ececff}#mermaid-1684818902147 text.actor{fill:#000;stroke:none}#mermaid-1684818902147 .actor-line{stroke:grey}#mermaid-1684818902147 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1684818902147 .messageLine0,#mermaid-1684818902147 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1684818902147 #arrowhead{fill:#333}#mermaid-1684818902147 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1684818902147 .messageText{fill:#333;stroke:none}#mermaid-1684818902147 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1684818902147 .labelText,#mermaid-1684818902147 .loopText{fill:#000;stroke:none}#mermaid-1684818902147 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1684818902147 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1684818902147 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1684818902147 .section{stroke:none;opacity:.2}#mermaid-1684818902147 .section0{fill:rgba(102,102,255,.49)}#mermaid-1684818902147 .section2{fill:#fff400}#mermaid-1684818902147 .section1,#mermaid-1684818902147 .section3{fill:#fff;opacity:.2}#mermaid-1684818902147 .sectionTitle0,#mermaid-1684818902147 .sectionTitle1,#mermaid-1684818902147 .sectionTitle2,#mermaid-1684818902147 .sectionTitle3{fill:#333}#mermaid-1684818902147 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1684818902147 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1684818902147 .grid path{stroke-width:0}#mermaid-1684818902147 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1684818902147 .task{stroke-width:2}#mermaid-1684818902147 .taskText{text-anchor:middle;font-size:11px}#mermaid-1684818902147 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1684818902147 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1684818902147 .taskText0,#mermaid-1684818902147 .taskText1,#mermaid-1684818902147 .taskText2,#mermaid-1684818902147 .taskText3{fill:#fff}#mermaid-1684818902147 .task0,#mermaid-1684818902147 .task1,#mermaid-1684818902147 .task2,#mermaid-1684818902147 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1684818902147 .taskTextOutside0,#mermaid-1684818902147 .taskTextOutside1,#mermaid-1684818902147 .taskTextOutside2,#mermaid-1684818902147 .taskTextOutside3{fill:#000}#mermaid-1684818902147 .active0,#mermaid-1684818902147 .active1,#mermaid-1684818902147 .active2,#mermaid-1684818902147 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1684818902147 .activeText0,#mermaid-1684818902147 .activeText1,#mermaid-1684818902147 .activeText2,#mermaid-1684818902147 .activeText3{fill:#000!important}#mermaid-1684818902147 .done0,#mermaid-1684818902147 .done1,#mermaid-1684818902147 .done2,#mermaid-1684818902147 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1684818902147 .doneText0,#mermaid-1684818902147 .doneText1,#mermaid-1684818902147 .doneText2,#mermaid-1684818902147 .doneText3{fill:#000!important}#mermaid-1684818902147 .crit0,#mermaid-1684818902147 .crit1,#mermaid-1684818902147 .crit2,#mermaid-1684818902147 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1684818902147 .activeCrit0,#mermaid-1684818902147 .activeCrit1,#mermaid-1684818902147 .activeCrit2,#mermaid-1684818902147 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1684818902147 .doneCrit0,#mermaid-1684818902147 .doneCrit1,#mermaid-1684818902147 .doneCrit2,#mermaid-1684818902147 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1684818902147 .activeCritText0,#mermaid-1684818902147 .activeCritText1,#mermaid-1684818902147 .activeCritText2,#mermaid-1684818902147 .activeCritText3,#mermaid-1684818902147 .doneCritText0,#mermaid-1684818902147 .doneCritText1,#mermaid-1684818902147 .doneCritText2,#mermaid-1684818902147 .doneCritText3{fill:#000!important}#mermaid-1684818902147 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1684818902147 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1684818902147 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1684818902147 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1684818902147 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1684818902147 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1684818902147 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1684818902147 #compositionEnd,#mermaid-1684818902147 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1684818902147 #aggregationEnd,#mermaid-1684818902147 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1684818902147 #dependencyEnd,#mermaid-1684818902147 #dependencyStart,#mermaid-1684818902147 #extensionEnd,#mermaid-1684818902147 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1684818902147 .branch-label,#mermaid-1684818902147 .commit-id,#mermaid-1684818902147 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1684818902147 {
    color: rgb(0, 0, 0);
    font: normal normal 400 normal 16px / normal "Times New Roman";
  }</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[an example of a blog post with diagrams]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="https://chojinie.github.io/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2021/distill</id><content type="html" xml:base="https://chojinie.github.io/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags.
An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>.
For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p>

<d-code block="" language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode.
You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="interactive-plots">Interactive Plots</h2>

<p>You can add interative plots using plotly + iframes :framed_picture:</p>

<div class="l-page">
  <iframe src="/assets/plotly/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<p>The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
  <span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
  <span class="n">df</span><span class="p">,</span>
  <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span>
  <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span>
  <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span>
  <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
  <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
  <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">assets/plotly/demo.html</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<hr />

<h2 id="details-boxes">Details boxes</h2>

<p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p>

<details><summary>Click here to know more</summary>
<p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p>
</details>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
⋅⋅* Unordered sub-list.</li>
  <li>Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list</li>
  <li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
  <li>Unordered list can use asterisks</li>
  <li>Or minuses</li>
  <li>Or pluses</li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry></feed>