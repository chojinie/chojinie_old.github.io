<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://chojinie.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://chojinie.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-06-04T05:06:56+00:00</updated><id>https://chojinie.github.io/feed.xml</id><title type="html">Cho HyunJin</title><subtitle>M.S student | Machine Learning Computer Vision
</subtitle><entry><title type="html">cs231n Summary - Optimization</title><link href="https://chojinie.github.io/blog/2023/cs231-Optimization/" rel="alternate" type="text/html" title="cs231n Summary - Optimization" /><published>2023-06-03T00:00:00+00:00</published><updated>2023-06-03T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/cs231-Optimization</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/cs231-Optimization/"><![CDATA[<h2 id="optimization">Optimization</h2>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic78-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic78-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic78-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic78.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>최적화는 주어진 모델과 손실 함수에 대해 가장 적절한 Parameter 값을 찾아내는 것으로, 학습 알고리즘을 사용하여 이를 수행합니다. 최적화는 모델을 훈련시키고 예측 성능을 개선하기 위해 필수적인 단계입니다.</p>

<h2 id="전략">전략</h2>

<h3 id="첫-번째-전략--random-search">첫 번째 전략 : Random Search</h3>
<p>최적화를 하기 위해 생각을 한 번 전개해보도록 하겠습니다. 손실슬 최소화 시키는 parameter(가중치 등)를 구하기 위해서 가장 간단한 아이디어는 단순하게도 최대한 많은 가중치를 모델에 대입해서 얼마나 정확도를 보이는지(손실 함수 작은지) 테스트해보는 것입니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># assume X_train is the data where each column is an example (e.g. 3073 x 50,000)
# assume Y_train are the labels (e.g. 1D array of 50,000)
# assume the function L evaluates the loss function

bestloss = float("inf") # Python assigns the highest possible float value
for num in range(1000):
  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters
  loss = L(X_train, Y_train, W) # get the loss over the entire training set
  if loss &lt; bestloss: # keep track of the best solution
    bestloss = loss
    bestW = W
  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)

# prints:
# in attempt 0 the loss was 9.401632, best 9.401632
# in attempt 1 the loss was 8.959668, best 8.959668
# in attempt 2 the loss was 9.044034, best 8.959668
# in attempt 3 the loss was 9.278948, best 8.959668
# in attempt 4 the loss was 8.857370, best 8.857370
# in attempt 5 the loss was 8.943151, best 8.857370
# in attempt 6 the loss was 8.605604, best 8.605604
# ... (trunctated: continues for 1000 lines)
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Assume X_test is [3073 x 10000], Y_test [10000 x 1]
scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples
# find the index with max score in each column (the predicted class)
Yte_predict = np.argmax(scores, axis = 0)
# and calculate accuracy (fraction of predictions that are correct)
np.mean(Yte_predict == Yte)
# returns 0.1555
</code></pre></div></div>

<p>이를 구현한 코드가 진행되면서 성능이 좋아지는 것을 볼 수 있습니다. 이를 통해 첫 번째 전략의 핵심 아이디어는 ‘반복’과 ‘개선’임을 알 수 있습니다. 임의의 W로 모델 테스트를 시작한 다음 반복하여 손실을 줄이는 것입니다. 하지만, 우리가 리신이 되어 등산로 입구로 내려가는 등산객이라고 한 번 생각해봅시다. CIFAR-10의 예에서 산에 있는 언덕의 개수가 W의 차원인 10 X 3072개 만큼 있을 겁니다… 어렵겠죠? 그만큼 이 방법은 좋지 않다는 것을 바로 느낄 수 있을 겁니다.</p>

<h3 id="두-번째-전략--random-local-search">두 번째 전략 : Random Local Search</h3>

<p>생각할 수 있는 첫 번째 전략은 임의의 방향으로 한 발을 뻗은 다음 내리막으로 이어지는 경우에만 발을 내딛는 것입니다.
구체적으로, 무작위로 선택한 초기 W로 시작하고, 이에 대해 무작위로 작은 움직임인 δW를 생성합니다. 그리고 만약 변동을 가한 W+δW에서의 손실이 더 낮다면, 우리는 더 좋은 가중치를 찾은 것으로 업데이트를 수행합니다. 즉 손실을 줄이는 방향으로 학습하는 것이죠.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W = np.random.randn(10, 3073) * 0.001 # generate random starting W
bestloss = float("inf")
for i in range(1000):
  step_size = 0.0001
  Wtry = W + np.random.randn(10, 3073) * step_size
  loss = L(Xtr_cols, Ytr, Wtry)
  if loss &lt; bestloss:
    W = Wtry
    bestloss = loss
  print 'iter %d loss is %f' % (i, bestloss)
</code></pre></div></div>
<p>하지만, 이것도 실제로 구현해 보면, 분류 정확도 21.4% 정도 밖에 얻지 못합니다. 또한 여전히 컴퓨팅 파워를 쓸데 없이 많이 잡아 먹습니다.</p>

<h3 id="세-번째-전략--following-the-gradient">세 번째 전략 : Following the Gradient</h3>

<p>좋은 방향을 무작위로 탐색할 필요는 없습니다. 손실을 가장 급격하게 감소시키는 방향을 수학적으로 계산하여 가중치 벡터를 변경해야 할 최적의 방향을 구할 수 있습니다. 이 방향은 손실 함수의 그래디언트와 관련이 있습니다. 두 번째 전략과 컨셉이 비슷하죠. 그래디언트는 입력 공간의 각 차원에 대한 기울기의 벡터(도함수:derivatives)입니다. 수학적 표현으로는 아래와 같이 나타낼 수 있습니다.</p>

<p>\begein{equation}
\frac{df(x)}{dx} = \lim{h \to 0}\frac{f(x+h)-f(x)}{h}
\end{equation}</p>

<p><strong>Gradient 계산방법</strong> <br />
그래디언트를 계산하는 방법에는 두 가지가 있습니다. 느리고 대략적이지만 쉬운 방법(Numerical 
그래디언트)과 빠르고 정확하지만 오류가 발생하기 쉬운 미적분학(Analytic 그래디언트)이 필요한 
방법입니다.</p>

<h4 id="computing-the-gradient-numerically-with-finite-differences">Computing the gradient numerically with finite differences</h4>
<p>위에 주어진 공식을 사용하면 그래디언트를 수치적으로 계산할 수 있습니다. 다음은 기울기를 평가하기 위해 함수 
f, 벡터 x를 사용하고 x에서 f의 기울기를 반환하는 일반 함수입니다. gradient는 함수가 가장 가파른 
증가율을 보이는 방향을 알려주지만 이 방향을 따라 얼마나 큰 보폭으로 움직여야하는지는 알려 주지 않습니다. 
이러한 보폭의 크기(학습 속도)를 선택하는 것은 신경망 훈련에서 가장 중요한 하이퍼 파라미터 설정 중 
하나입니다.</p>

<p><strong>효율성의 문제</strong>
예제에서 총 10 X 3073개의 매개변수를 가지고 있었기 때문에 그래디언트를 평가하고 단일 parameter 
업데이트 수행을 위해 손실 함수의 30731개 평가를 수행해야 했습니다. 하지만 변수가 수천만개에 달한다면 
계산하는 데에 정말 많은 비용이 들것입니다. 개선이 필요하겠죠.</p>

<h4 id="computing-the-gradient-analytically-with-calculus">Computing the gradient analytically with Calculus</h4>

<p>Numerical 방식은 finite difference approximation을 이용하여 계산하기 때문에 매우 
심플합니다. 다만 정확한 값은 아니며 approimate 값이라는 점, 컴퓨팅에 매우 많은 비용이 든다는 아쉬운 
점이 있습니다. 그래서 두 번째 방식으로는 Calculus를 이용하는 분석적 방법으로 gradient를 구하게 
됩니다. 이 경우 빠르게 계산을 할 수 있게 됩니다. 빠른만큼 더 많은 오답이 발생할 수 있습니다. 그래서 
정확성을 평가하기 위해 numerical gradient와 결과를 비교하게 되며, 이를 Gradient check라고 
합니다.</p>

<p>
그래디언트 계산에 하나의 에시를 들어 보겠습니다. 하나의 데이터 포인트에 대한 SVM 손실 함수를 사용하겠습니다.
\begin{equation}

L_i = \sum{j \ne y_i}[max(0, {w_j}^T{x_i} - {w_{y_i}}^T{x_i} + \Delta)]

\end{equation}

가중치인 w에 대하여 함수 간에 차를 구할 수 있습니다. 아래 예시는 $$ w{y_i} $$에 대한 
gradient를 구해 봤습니다.

\begin{equation}
\nabla{w{y_i}}L_i = -(\sum{j \ne y_i}\mathbb{1}({w_j}^T{x_i} - 
{w{y_i}}^T{x_i} + \Delta &gt; 0)x_i
\end{equation}


$$ \mathbb{1} $$은 내부 조건이 참이면 1이고 그렇지 않으면 0인 indicator function입니다.
주어진 문장은 정확한 클래스에 해당하는 W의 행에 대한 기울기만을 설명하며, 코드로 구현할 때는 원하는 
마진을 충족하지 못한 클래스의 개수를 간단히 계산하는 것으로 이해하시면 됩니다. 원하는 마진을 충족하지 
못한 클래스의 개수에 따라 데이터 벡터 xi를 해당 개수로 곱하여 스케일링한 값이 기울기가 됩니다. 주어진 
내용은 올바른 클래스에 해당하는 W의 행에 대한 기울기만을 설명합니다. 다른 행에 대한 기울기는 제시되지 
않습니다.

다른 행(j≠yi)에 대한 기울기는 다음과 같습니다:

\begin{equation}
\nabla{w{y_i}}L_i = \mathbb{1}({w_j}^T{x_i} - 
{w{y_i}}^T{x_i} + \Delta &gt; 0)x_i
\end{equation}
</p>

<h3 id="gradient-descent-경사-하강법">Gradient Descent (경사 하강법)</h3>

<p>이제 손실 함수의 그래디언트를 계산할 수 있게 되었습니다. 그래디언트를 반복적으로 평가한 다음 매개변수 
업데이트를 수행하는 절차를 경사 하강법(Gradient Descent)라고 합니다. <strong>바닐라</strong> 버전은 
다음과 같습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Vanilla Gradient Descent

while True:
  weights_grad = evaluate_gradient(loss_fun, data, weights)
  weights += - step_size * weights_grad # perform parameter update
</code></pre></div></div>
<p>경사 하강법은 현재까지 신경망 손실 기능을 최적화하는 가장 일반적이고 확립된 방법입니다.<br /></p>

<p><strong>Mini-batch gradient descent</strong>. 대규모 애플리케이션(예: ILSVRC 챌린지)에서 훈련 데이터는 
수백만 개의 example을 가질 수 있습니다. 따라서 하나의 매개변수 업데이트만 수행하기 위해 전체 훈련 
세트에 대해 전체 손실 함수를 계산하는 것은 낭비인 것 같습니다. 이 문제를 해결하는 가장 일반적인 접근 
방식은 데이터 셋의 일부분인 ‘배치’에 대한 기울기만을 계산하는 것입니다. 예를 들어, 최신 
ConvNets에서 일반적인 배치에는 120만 개의 전체 교육 세트에서 256개의 example이 포함됩니다. 그런 
다음 이 배치를 사용하여 매개변수 업데이트를 수행합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Vanilla Minibatch Gradient Descent

while True:
  data_batch = sample_training_data(data, 256) # sample 256 examples
  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  weights += - step_size * weights_grad # perform parameter update
</code></pre></div></div>
<p>이것이 잘 작동하는 이유는 훈련 데이터의 예가 서로 연관되어 있기 때문입니다. ILSVRC의 120만 이미지 
모두가 실제로 단 1000개의 고유한 이미지(각 클래스에 대해 하나씩, 즉 각 이미지의 동일한 복사본 
1200개)의 정확한 복제본으로 구성되는 극단적인 경우를 고려해보겠습니다. 1200개의 동일한 복사본 모두에 
대해 계산할 그래디언트는 동일하게 됩니다.그리고 120만개의 이미지 전체에 대한 데이터 손실을 평균화하면 
1000개의 작은 subset에서만 평가한 것과 정확히 동일한 손실이 발생합니다. 물론 실제로 데이터 세트에는 
중복 이미지가 포함되지 않으며 미니 배치의 그래디언트는 전체 목적의 그래디언트에 대한 좋은 근사치로 
여기는 것입니다. 따라서 더 자주 매개변수 업데이트를 수행하기 위해 미니 배치 기울기를 평가하여 실제로 
훨씬 더 빠른 수렴을 달성할 수 있습니다.</p>

<h4 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h4>
<p>이것의 극단적인 경우는 미니 배치가 하나의 example만 포함하는 설정입니다. 이 프로세스를 Stochastic 
Gradient Descent(SGD)(또는 때때로 온라인 Gradient Descent)라고 합니다. 실제로는 벡터화된 
코드 최적화로 인해 하나의 example에 대한 그래디언트를 100번 평가하는 것보다 100개의 example에 
대한 그래디언트를 평가하는 것이 훨씬 더 계산적으로 효율적일 수 있기 때문에 이것은 상대적으로 덜 
일반적입니다. 하지만 사람들은 미니배치 경사 하강법을 참조할 때에도 일반적으로 “SGD”라는 용어를 
사용합니다. (즉, “Minibatch Gradient Descent”를 나타내는 MGD나 “Batch gradient 
descent”를 나타내는 BGD와 같은 용어는 드물게 사용됩니다.) 보통 미니배치가 사용된다고 가정합니다. 
미니배치의 크기는 하이퍼파라미터이지만 이를 교차 검증하는 것은 그리 일반적이지 않습니다.</p>

<h2 id="참고">참고</h2>
<p>http://cs231n.stanford.edu/schedule.html</p>]]></content><author><name></name></author><category term="study" /><category term="cs231" /><category term="AI" /><category term="cs231" /><category term="study" /><category term="AI" /><summary type="html"><![CDATA[recording_a Summary of lecture]]></summary></entry><entry><title type="html">cs231n Summary - Regularization</title><link href="https://chojinie.github.io/blog/2023/cs231-regularization/" rel="alternate" type="text/html" title="cs231n Summary - Regularization" /><published>2023-06-03T00:00:00+00:00</published><updated>2023-06-03T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/cs231-regularization</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/cs231-regularization/"><![CDATA[<h2 id="배경">배경</h2>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic77-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic77-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic77-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic77.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>손실 함수에는 한가지 오류가 있습니다. 이를 위해 Parameter set “W”를 갖는 데이터셋이 있다고 가정해봅시다. W는 모든 example에 대해서 정확히 분류해냅니다. 모든 스코어 점수는 모든 마진을 충족하기 대문에 \(L_i = 0 \; \text{for all} \; i\) 라고 둘 수 있습니다. 여기서 발생하는 문제는 W의 set이 반드시 하나만이라고 볼 수 는 없다는 점입니다.</p>

<p>간단히 예시를 들어 이해해 보겠습니다. 매개 변수 W가 모든 example을 올바르게 분류하여 example에 대한 loss가 0이 나온다면, parameter의 임의의 상수가 곱해진 \(\lambda W (\lambda &gt; 1)\) 도 loss가 0이 됩니다. 왜냐하면 이 변환은 모든 점수의 크기를 균등하게 늘리기 때문에 그들의 절대적인 차이도 균등하게 늘어납니다. 예를 들어, 올바른 클래스와 가장 가까운 잘못된 클래스 사이의 점수 차이가 15였다면, W의 모든 요소를 2배로 곱하면 새로운 차이는 30이 됩니다. 이러한 모호성을 없애기 위해 특정 가중치 세트 W에 대한 선호도(preference)를 넣어줘야 합니다.</p>

<h2 id="방법론">방법론</h2>

<p>손실함수에 regularization penalty를 줌으로써 구현할 수 있습니다. 가장 보편적인 regularization penaly는 L2 norm 형태입니다. 가중치의 제곱을 더해서 큰 가중치에 패널티를 주는 방식으로 작동합니다. L2 norm을 최소화하는 것은 가중치 값을 작게 유지하는 것을 의미합니다. 따라서 큰 가중치를 줄이고 작은 가중치를 선호하는 경향이 생기게 됩니다. 이는 모델이 더 일반화되고 오버피팅을 피할 수 있도록 도와줍니다.</p>

<p>\begin{equation} \mathbf{R(W)} = \sum_{k}\sum_{l}(W_{k,l})^2 \end{equation}</p>

<p>위 식에서 W의 모든 요소를 제곱하여 더합니다. 정규화 함수는 데이터에 의존하는 것이 아니라 가중치에만 기반하므로 데이터와는 무관합니다. 정규화 페널티를 포함하여 전체 다중 클래스 SVM loss를 완성할 수 잇으며, 이는 데이터 손실과 정규화 손실로 구성됩니다. 즉, SVM은 다음과 같이 표현할 수 있습니다.</p>

<p>\begin{equation} L = \frac{1}{N}\sum_i{L_i} + \lambda R(W) \end{equation}</p>

<h2 id="l2-regularization의-장점">L2 regularization의 장점</h2>

<p>L2 norm을 사용하면 SVM에서 최대 마진 속성으로 이어지게 됩니다. 또 다른 이점은 큰 가중치에 페널티를 줘서 모델의 일반화를 개선한다는 것입니다. 이는 어떤 입력 차원도 그 자체로 점수에 매우 큰 영향을 미칠 수 없음을 의미하기 때문입니다. 예를 들어 입력 벡터 \(\vec{x} = [1,1,1,1]\) 이고, 두개의 가중치 벡터 \(\vec{w_1} = [1,0,0,0]\) , \(\vec{w_2} = [0.25,0.25,0.25,0.25]\) 가 있다고 가정하겠습니다. 이 경우 \(\vec{w_1}^T{\vec{x}} = \vec{w_2}^T{\vec{x}} = 1\)로 동일한 것을 알 수 있습니다. 하지만 \(\vec{(w_1)}\)의 L2 penalty는 1.0인 반면 \(\vec{w_2}^T{\vec{x}}\) 의 것은 0.5일 뿐입니다.( \(0.25^2 + 0.25^2 + 0.25^2 + 0.25^2 = 0.25,\) L2 Norm을 계산하게 되면 최종적으로 루트를 씌우기 때문에) 그래서 L2 패널티에 따르면, 가중치 벡터 w2가 더 낮은 정규화 손실을 가지기 때문에 선호됩니다. 직관적으로 이해하면, w2의 가중치는 더 작고 퍼져있기 때문입니다.</p>

<h2 id="참고">참고</h2>
<p>http://cs231n.stanford.edu/schedule.html</p>]]></content><author><name></name></author><category term="study" /><category term="cs231" /><category term="AI" /><category term="cs231" /><category term="study" /><category term="AI" /><summary type="html"><![CDATA[recording_a Summary of lecture]]></summary></entry><entry><title type="html">norm</title><link href="https://chojinie.github.io/blog/2023/math-norm/" rel="alternate" type="text/html" title="norm" /><published>2023-06-01T00:00:00+00:00</published><updated>2023-06-01T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/math-norm</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/math-norm/"><![CDATA[<h2 id="norms">Norms</h2>

<h3 id="정의">정의</h3>

<p>vector space  \(V\)  상에서의 함수 혹은 연산 값을 norm이라고 한다.</p>

<p>그 함수 혹은 연산은  \(\left\vert\left\vert \cdot\right\vert\right\vert : V\rightarrow\mathbb{R}\)  와 같이 연산된 값이 하나의 스칼라 값으로 나타나게 된다.</p>

<p>즉, 간단히 설명하자면 vector  \(\mathbf{x}\) 의 길이는 norm으로 표현되고,  \(\left\vert\left\vert \mathbf{x}\right\vert\right\vert \in \mathbb{R}\) 로 나타낼 수 있다.</p>

<h3 id="조건">조건</h3>
<ul>
  <li>
    <p>Homogeneous :  \(\lVert\lambda\mathbf{x} \rVert = \left\vert \lambda \right\vert\lVert\mathbf{x} \rVert\) 
벡터에 스칼라 값을 곱한 것을 norm을 취하나, norm을 취한 벡터에 스칼라 값을 곱하나 결과는 같다.</p>
  </li>
  <li>
    <p>Triangle inequality :  \(\lVert \mathbf{x+y}\rVert\le\lVert \mathbf{x}\rVert +\lVert \mathbf{y}\rVert\) 
<img src="https://velog.velcdn.com/images/jinnij/post/66e07f5e-5adf-48e8-b1e6-04a02653fda7/image.png" alt="WikipediA" /></p>
  </li>
  <li>
    <p>Positive Definite :  \(\lVert \mathbf{x}\rVert\ge0\)  그리고,  \(\lVert \mathbf{x}\rVert= 0\) 이라고 함은,  \(\mathbf{x}=0\) 일 때를 의미한다.
(앞으로 나올 부등호에서 0을 포함한 것은 중요한 포인트이다. x=0을 만족해야만 한다.)</p>
  </li>
</ul>

<h3 id="다양한-norms">다양한 Norms</h3>

<p>General form으로 P-norm이라고 지칭한다.</p>
<ul>
  <li>notation :  \(\lVert\mathbf{x_p}\rVert\equiv\sqrt[p]{\sum_{i=1}^nx_i^p}\)</li>
</ul>

<h4 id="manhattan-norml1-norm">Manhattan Norm(L1-norm)</h4>
<p>\(\lVert\mathbf{x_1}\rVert\equiv\sum_{i=1}^n\left\vert x_i\right\vert\) 
<img src="https://velog.velcdn.com/images/jinnij/post/e16142c3-3aad-47e2-abc9-6b71af95cc19/image.png" alt="" /></p>

<h4 id="euclidean-norml2-norm">Euclidean Norm(L2-norm)</h4>
<p>\(\lVert\mathbf{x_p}\rVert\equiv\sqrt{\sum_{i=1}^nx_i^2}=\sqrt{\mathbf{x^T}\mathbf{x}}\) 
<img src="https://velog.velcdn.com/images/jinnij/post/3c1028c6-d545-436b-88e0-477e74c75403/image.png" alt="" /></p>

<p>이후에는 학습적으로 L2-norm을 “norm”이라고 간주한다.(단, 논문이나 기타 공식적인 문서를 예외다. 정확히 무슨 norm인지 표시해줘야 한다.)</p>

<h2 id="dot-product">Dot Product</h2>

<h3 id="정의-1">정의</h3>
<p>\(\mathbf{x\cdot y}\equiv\mathbf{x^T\cdot y}=\sum_{i=1}^nx_iy_i\)</p>

<p>즉, 스칼라 곱이라고 생각하면 쉽다. 두 벡터의 곱이 하나의 스칼라 값이 나온다. 앞으로는 세 번째 수식이 아닌 두 번째 수식에 익숙해져야 한다.</p>

<h4 id="dot-product--neq--inner-product내적">Dot Product  \(\neq\)  Inner Product(내적)</h4>

<p>사실, Dot Product  \(\in\)  (General) Inner Product 관계이다.</p>

<h2 id="bilinear-mapping">Bilinear mapping</h2>

<h3 id="정의-2">정의</h3>
<p>두개의 arguments(벡터)를 받아서 출력(매핑)시키는 것을 뜻한다.(i.e.  \(\mathbf\Omega(x,y)\) )</p>

<h3 id="조건-1">조건</h3>
<p>두 가지 조건을 모두 만족해야 한다.</p>
<ul>
  <li>\(\mathbf{\Omega(\lambda x+\psi y,z)=\lambda\Omega(x,z)+\psi\Omega(y,z)}\) 
‘스칼라를 곱하고 더한 것’의 매핑과 ‘매핑하고 스칼라를 곱하고 더한 것’의 값이 같아야 한다.</li>
  <li>\(\mathbf{\Omega( x,\lambda y+\psi z)=\lambda\Omega(x,y)+\psi\Omega(x,z)}\)<br />
Bilinear하므로 뒤에서 한 것의 값도 같아야 한다.</li>
</ul>

<h2 id="symmetric--positive-definite">Symmetric &amp; Positive Definite</h2>

<p>Bilinear mapping이 특정 조건을 만족할 경우, 어떠한 특성을 가지고 있는 Bilinear mapping이라고 설명할 수 있다.</p>

<p>단, Bilinear mapping을  \(\mathbf{V}\) X \(\mathbf{V}\rightarrow\mathbb{R}\)  인 경우로 한정한다. 즉 두 벡터 스페이스에서 각각의 벡터를 뽑아서 연산했을 때, 하나의 스칼라 값이 나오는 경우로 한정 짓는다.</p>

<ul>
  <li>Bilinear mapping이 Symmetric하다
( \(\Omega\)  is “Symmetric”)고 표현되려면,
If  \(\mathbf{\Omega(x,y)=\Omega(y,x)}\)  for all  \(\mathbf{x,y}\in\mathbf{V}\)</li>
</ul>

<p>i.e.) 내적  \(x\cdot y=y\cdot x\)</p>

<ul>
  <li>Bilinear mapping이 Positive Definite하다( \(\Omega\)  is “Positive Definite”)고 표현되려면,
If  \(\forall\mathbf{x}\in\mathbf{V}-\{0\}: \mathbf{\Omega(x,x)&gt;0}\)  and  \(\mathbf{\Omega(0,0)=0}\) 
전체 벡터 스페이스에서 0이 제외되었을 때는 Bilinear mapping 값이 0보다 클 때, 그리고 (0,0)의 Bilinear mapping이 0일 때
(즉, 0을 제외해서는 결과 값이 0이 되면 안되고 0일 경우에는 결과 값이 0이 될 때)</li>
</ul>

<h2 id="general-inner-product">General Inner Product</h2>

<h3 id="정의-3">정의</h3>

<p>Positive definite 하고 Symmetric한 bilinear mapping을 의미한다.</p>

<p>Notation :  \(\mathbf{&lt;x,y&gt;}\) 
i.e : 내적</p>

<h3 id="inner-product-space">Inner Product Space</h3>
<p>\(\mathbf(V,&lt;\cdot,\cdot&gt;)\) 의 페어를 의미한다 &lt;&gt;는 연산을 의미한다.
i.e :  \(\mathbf{V}\)  가  \(\mathbb{R}^n\) 이고, &lt;&gt;이 dot product라면,
이 때의 Inner Product Space를 ‘Uclidean vector space’라고 한다.</p>

<h2 id="inner-product이지만-dot-product가-아닌-경우">Inner Product이지만 Dot Product가 아닌 경우</h2>

<h3 id="예시">예시</h3>
<p>\(\mathbf{V}=\mathbb{R^2}\) 
 \(\mathbf{&lt;x,y&gt;}\equiv x_1y_1-(x_1y_2+x_2y_1)+2x_2y_2 \in \mathbb{R}\)</p>

<p>만약 Dot Product라면  \(x_1y_1+x_2y_2\)  형식을 띄게 된다.</p>

<p>1) Bilinear Mapping인가? <em>Yes</em>
 \(\because \mathbf{V}\)  X  \(\mathbf{V}\rightarrow \mathbb{R}\)</p>

<p>2) Symmetric한 B.M 인가? <em>Yes</em>
 \(\because x_1y_1-(x_1y_2+x_2y_1)+2x_2y_2= y_1x_1-(y_1x_2+y_2x_1)+2y_2x_2 \iff \mathbf{&lt;x,y&gt;=&lt;y,x&gt;}\)</p>

<p>3) Positive Definite한 B.M 인가? <em>Yes</em></p>

<p>\(\because \mathbf{&lt;0,0&gt;}=0 \\ \mathbf{&lt;x,x&gt;}\)  &gt; 0  \((\mathbf{V}-\{0\})\) 
 \(x_1, x_2\) 를 넣어보고 식을 정리하면 아래 식을 얻을 수 있고 0보다 크다는 것을 알 수 있다.</p>

\[(x_1-x_2)^2+x^2 &gt;0\]

<h2 id="derivation-of-matrix-of-inner-product">Derivation of Matrix of Inner Product</h2>

<p>Inner product를 매트릭스의 형태로 변형시키는 과정을 알아보자.</p>

<p>inner product : Symmetric &amp; Positive Definite</p>

<p>ordered basis B =  \(b_1, \cdots, b_n\)</p>

<p>basis의 linear combination으로 벡터 스페이스 상의 모든 벡터를 표시할 수 있고</p>

<p>\(\psi, \lambda\) 를 coordinate로 표시할 수 있다. (Coordinate with respect to Basis)
 \(\mathbf{x^{'}}=[\psi_1,\cdots,\psi_n]^T\) 
 \(\mathbf{y^{'}}=[\lambda_1,\cdots,\lambda_n]^T\)</p>

<p>inner product의 bilinearity를 사용한다면,</p>

\[&lt;x,y&gt;=&lt;\sum_{i=1}^n\psi_i\mathbf{b}_i,\sum_{i=1}^n\lambda_i\mathbf{b}_i&gt;=\sum_{i=1}^n\sum_{i=1}^n\psi_i&lt;\mathbf{b_i,b_j}&gt;\lambda_j = \mathbf{x^{'T}Ay^{'}} \in \mathbb{R}\]

\[A_{ij}\equiv&lt;b_i,b_j&gt;\]

<p>A는 basis에 대해서 i,j에 element를 갖는 스칼라로 구성된 매트릭스가 된다.</p>

<p>basis만 존재한다면, 벡터 스페이스에서 추출한 모든 벡터에 대해 A 매트릭스 하나만으로 inner product를 계산할 수 있게 되는 것이다.</p>

<h2 id="symmetric--positive-definite-matrix">Symmetric &amp; Positive Definite Matrix</h2>

\[A_{ij} \equiv &lt;\mathbf{b_i,b_j}&gt;\]

<h3 id="특성">특성</h3>
<ul>
  <li>Symmetric Matrix
 \(\because A_{ij}=&lt;b_i,b_j&gt;=&lt;b_j,b_i&gt;=A_{ji}\)</li>
  <li>Positive Definite Matrix
 \(\forall \mathbf{x}-\{0\}: \mathbf{x^TAx}&gt;0\)</li>
  <li>
    <p>Null space of A  \((\in \mathbb{R})\) 는 오직 0으로만 구성되어 있다.
 \(\mathbf{x^TAx}&gt;0 \ (\forall \mathbf{x}\neq0)\) 
null space는 기본적으로  \(A\mathbf{x}=0\) 성립하는 x의 non trivial solution을 찾아야 한다. x가 0이 아니면 식이 성립을 안하기 때문에
Null space는 x=0인 trivial solution 외의 값은 존재할 수 없다는 의미.</p>
  </li>
  <li>\(A\) 의 Diagonal element는 0보다 크다(positive)</li>
</ul>

<h3 id="additionalif-possible">Additional(If possible,)</h3>
<ul>
  <li>Positive Semi-definite Matrix
 \(\forall \mathbf{x}-\{0\}: \mathbf{x^TAx}\ge0\)</li>
</ul>

<h2 id="lengths-and-distances">Lengths and Distances</h2>

<ul>
  <li>
    <p>Norm &amp; Inner product
 \(\lVert \mathbf{x}\rVert\equiv\sqrt{&lt;\mathbf{x,x}&gt;}\) 
Norm = Inner product에 square(루트)를 씌운 것. (length)</p>
  </li>
  <li>
    <p>Cauchy-Schwarz Inequality
 \(|\mathbf{&lt;x,y&gt;}|\le\lVert\mathbf{x}\rVert\lVert\mathbf{y}\rVert\)</p>
  </li>
  <li>
    <p>Distance
두 점 간의 norm 형태</p>
  </li>
</ul>]]></content><author><name></name></author><category term="study" /><category term="math" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[recording_a Summary of math]]></summary></entry><entry><title type="html">support vector machine(SVM)</title><link href="https://chojinie.github.io/blog/2023/support-vector-machine/" rel="alternate" type="text/html" title="support vector machine(SVM)" /><published>2023-06-01T00:00:00+00:00</published><updated>2023-06-01T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/support-vector-machine</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/support-vector-machine/"><![CDATA[<p>처음 접하게 된 SVM을 정확히 이해하려고 노력했습니다. UNIST 유재준 교수님의 블로그(하단의 참고에 출처)와 Mathmatic for ML교재를 바탕으로 공부 내용을 정리하고자 합니다. SVM은 매우 아름답고 탄탄한 이론적인 배경을 바탕(Steinwart and Christmann, 2008)으로 정교하게 고안된 기계학습 알고리즘이며, 실제 적용이 여러 모로 쉽고 성능이 강력하며 따라서 실전적이라는 점이 매력적이라고 합니다.</p>

<h2 id="classification-with-support-vector-machines">Classification with Support \Vector Machines</h2>

<p>많은 경우에 우리는 다양한 선택지 중에서 옳은 답을 결정하는 머신러닝 알고리즘을 바랄 것입니다. 예를 들어, 스팸 메일 분류 기능에서는 스팸 메일과 정상 메일을 분류해 낼 수 있죠. 이렇게 오직 두개의 결과만을 바라 보고 어떤 결과에 해당되는지를 분류해내는 분야를 Binary Classification(이진 분류)이라 합니다. 출력이 분류값으로 가질 수 있는 오직 두개의 클래스를 {+1, -1 } 으로 나타내겠습니다. 아래와 같은 수식으로 표현하게 됩니다.</p>

<p>\begin{equation} f : \mathbb{R}^D \rightarrow { +1, -1 } \end{equation}</p>

<p>각각의 example (데이터 포인트) \(x_n\) 은 D개의 실수로 구성된 피처 벡터로 나타냅니다. 레이블은 일반적으로 양성 클래스(+1)와 음성 클래스(-1)로 구분됩니다. 이진 분류작업에 사용되는 SVM (Support \Vector Machine)이라고 하는 접근 방식을 소개하겠습니다.</p>

<p>SVM이 풀고자하는 문제는 다음과 같습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"How do we divide the space with decision boundaries?"
</code></pre></div></div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/svm/pic4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/svm/pic4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/svm/pic4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/svm/pic4.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>위 그림에서 보면 ‘+’ example과 ‘-‘ example을 어떻게 구별 할 수 있을까?
어떻게 최대한 그 둘 간의 구분을 지어 놓을까에 대해 고민하는 것으로 해석 할 수 있겠습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/svm/pic5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/svm/pic5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/svm/pic5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/svm/pic5.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>이런식으로 -와 + example이 가장 맞닿아 있는 부분들(노란 실선)로 부터 특정 간격만큼의 거리가 벌려져서 가장 넓어 질 수 있는 라인(빨간 점선)을 정하면 될 것 같습니다.</p>

<details>
<summary>hyperplane 추가 설명 펼치기/접기</summary>

Hyperplane은 차원이 D - 1인 (해당하는 벡터 공간이 차원 D인 경우) 아핀 sub space입니다. 이 예제들은 두 개의 클래스로 구성되어 있으며 (두 가지 가능한 레이블이 있음), 이들을 직선으로 그려서 분리/분류할 수 있도록 특징들 (example을 나타내는 벡터의 구성 요소들)이 배열되어 있습니다.

</details>

<details>
<summary>SVM 사용 이유 추가 설명 펼치기/접기</summary>

살짝 복잡하게 말해보면, 이는 회귀와 마찬가지로 binary 레이블 $$ y_n \in \{+1, -1\} $$ 와 짝을 이루는 example $$ x_n \in \mathbb{r}^D $$ 의 집합에서 지도 학습 task를 갖고 있습니다. example-레이블 쌍 {(x1, y1), ..., (xN, yN)}로 구성된 훈련 데이터 세트가 주어졌을 때, 최소의 분류 오류를 얻는 모델의 매개변수를 추정하는 것 입니다. 선형/비선형 모델을 모두 고려해야하지만, 당장은 선형 모델만을 고려하겠습니다.

이진 분류를 SVM을 사용하여 설명하는 데에는 두 가지 주요 이유가 있습니다. 첫째, SVM은 지도 학습의 기하학적인 관점을 고려할 수 있게 해줍니다. 두 번째는 SVM의 최적화 문제가 해석적인 해를 가지지 않아 다양한 최적화 도구를 활용해야 한다는 점입니다.

SVM의 기계 학습 관점은 최대 우도 관점과 약간 다릅니다. 최대 우도 관점은 데이터 분포의 확률적인 관점을 기반으로 모델을 제안하고, 이를 기반으로 최적화 문제를 도출합니다. 반면, SVM 관점은 기하학적 직관에 기반하여 훈련 중에 최적화되어야 하는 특정한 함수를 설계하는 것으로 시작합니다. SVM의 경우, 훈련 데이터에서 최소화되어야 하는 손실 함수를 설계하기 시작합니다. 이는 경험적 위험 최소화 원칙을 따릅니다.

</details>

<h3 id="decision-rule">Decision Rule</h3>

<p>점선(decision boundary)를 정하기 위해 decision rule에 대한 설정이 선행되어야 합니다. example과 점선 간의 관계를 나타내기 가장 쉬운 방법은 “거리”, “방향”과 관련된 식을 도출해내는 것입니다. 그렇기 때문에 \(\vec{w}\) 를 하나 그려보겠습니다. 간격의 중심선(빨간 점선)에 대해 직교하는 벡터입니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/svm/pic6-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/svm/pic6-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/svm/pic6-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/svm/pic6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>그리고 임의의 example \(\vec{u}\) 가 있을 때 간격을 기준으로 오른쪽에 속할지 왼쪽에 속할지를 알아내야 합니다. 여기서 내적의 개념을 생각해 볼 수 있습니다. 내적은 하나의 벡터가 다른 벡터로 projection하는 것으로 이해할 수 있죠.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/svm/pic7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/svm/pic7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/svm/pic7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/svm/pic7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>\(\theta\) 만큼의 각도를 가지며 \(\vec{a}\)와 \(\vec{b}\)가 내적하는 경우를 상상해봅시다. 만약 두 벡터가 X,Y평면에 있다고 생각하면 \(\vec{a}\)는 x, y 성분으로 분리할 수 있겠죠. \(\vec{b}\) 가 x축이라고 여기면, projection된 a는 b벡터 방향(x축 방향)의 x성분 만큼의 크기를 갖는 \(\left\vert a \right\vert cos\theta\)의 벡터 형태로 변하게 됩니다. 이를 b와 곱한 것이 내적의 표현입니다.</p>

<p>다시 \(\vec{w}\), \(\vec {u}\)의 입장으로 돌아가 보겠습니다. \(\vec {u}\)는 \(\vec {w}\)와 \(\theta\) 만큼 각도를 가지며 u는 w방향 성분만큼 projection되어 \(\left\vert u \right\vert cos\theta\) 벡터로 변할 것입니다. 그리고 \(\vec{u} \cdot \vec{v} = \left\vert u \right\vert \left\vert v \right\vert cos\theta\) 의 크기를 갖는 선이 될 것입니다. 그 선의 크기(길이)의 의미는 그래프의 원점으로 부터 간격이 있는 방향으로 얼마만큼 멀리 떨어져 있느냐를 얘기합니다.</p>

<p>길이가 너무 길어서 간격을 벗어나면 ‘+’, 너무 짧아서 간격에 못미치면 ‘-‘를 분류해 내는 것입니다. 임의의 상수 b와 크기를 비교하는 식으로 일반화 시킬 수 있습니다.
\begin{equation}
\vec{w} \cdot \vec{u} + b \ge 0 \qquad then \; \text{‘+’}
\end{equation}</p>

<p>하지만 일반화 식에서 \vec{w} 와 b는 어떤 값을 잡아야하는지 전혀 알 수 가 없습니다. under constraint 상황이기 때문에 constraint (제약) 조건을 추가하는 작업을 아래에서 해 나갈 예정입니다.</p>

<h3 id="design-and-add-additional-constraints">Design and Add additional constraints</h3>

<p>위 식을 각 클래스 별로 식으로 표현해보겠습니다.</p>

<p>\begin{equation} \vec{w}{x_+} + b \ge 1
\end{equation}</p>

<p>\begin{equation} \vec{w}{x_-} + b \le -1
\end{equation}</p>

<p>즉 decision rule이 최소한 1보다 크거나 -1보다 작은 값을 주도록 해봤습니다. 두 개의 식을 하나의 변수를 추가하여 하나의 식으로 변경해 보도록 하죠.(변수를 추가하는 수학적 의미는 거창한게 아니라 편하려고 도입한 것입니다.)</p>

<p>\begin{equation}
y_i =
\begin{cases}
    1 &amp; \text{for ‘+’ }<br />
    -1 &amp; \text{for ‘-‘ }
\end{cases}
\end{equation}</p>

<p>\(y_i\) 를 (3) 식에 각각 곱해 줍니다.
아래와 같이 두개의 식을 하나의 식으로 묶어서 표현할 수 있게 됩니다.</p>

<p>\begin{equation}
y_i ( \vec{w} \cdot \vec{x_+} + b ) \ge 1
\end{equation}</p>

<p>1을 좌변으로 옮기면 다음과 같이 표현할 수 있게 됩니다.</p>

<p>\begin{equation}
y_i ( \vec{w} \cdot \vec{x_+} + b ) -1 \ge 0
\end{equation}</p>

<p>여기서 등호가 성립할 때는 example \(x_i\) 가 노란선(경계)에 정확히 걸칠 경우입니다.</p>

<p>\begin{equation}
y_i ( \vec{w} \cdot \vec{x_i} + b ) -1 = 0 \qquad for \; \vec{x_i} \in 노란선
\end{equation}</p>

<p>이제 간격의 크기(벡터의 특정 구간 길이)를 한번 구해봅시다. \(\vec{x_+}\)와 \(\vec{v_-}\) 벡터들을 각각 가정해봅시다. 각각 ‘+’ ‘-‘ example 입니다. 두 벡터 간의 차이를 구한다음 프로젝션 시키면, 벡터간의 거리를 구할 수 있겠죠. 아래처럼 식으로 표현할 수 있습니다. 간격(WIDTH)은 마진(margin)이라고 표현하기도 합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/svm/pic8-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/svm/pic8-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/svm/pic8-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/svm/pic8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>\begin{equation}
WIDTH = (x_+ \; - x_-) \; \cdot \; \frac{\vec{w}}{\lVert \vec{w} \rVert}
\end{equation}</p>

<p>위의 노란선 위에 example이 놓였을 때의 수식 덕분에 WIDTH의 분자를 계산하면 아래와 같이 표현할 수 있습니다.</p>

<p>\begin{equation}
WIDTH = \frac{2}{\lVert \vec{w} \rVert}
\end{equation}</p>

<p>두 클래스를 나누는 선형 separator를 찾기 위한 아이디어를 공식화 해보려고합니다. margin(마진)의 개념을 소개한 다음, 분류 에러를 야기하여 example을 “wrong” 부분에 놓이게 하도록 선형 separator의 개념을 확대하겠습니다. 두 가지 관점에서의 동등한 SVM을 공식화하는 방식을 얘기할 것입니다. : 첫 번째는 geometric 관점이며, 두 번째는 loss function 관점입니다. Lagrange multipliers를 이용하여 SVM의 dual version을 유도할 것입니다. dual SVM은 SVM 공식화의 세 번째 관점을 제공합니다.(각 클래스의 example의 convex hulls로 표현하는 관점) 이 후에는 커널(kernels)에 대해 간략히 설명한 후, 비선형 커널 SVM 최적화 문제를 수치적으로 어떻게 해결하는 지에 대한 설명으로 마무리 하겠습니다.</p>

<h2 id="참고">참고</h2>
<p>MATHEMATICS FOR MACHINE LEARNING(1st Edition) by Marc Peter Deisenroth
https://jaejunyoo.blogspot.com/search?q=svm</p>]]></content><author><name></name></author><category term="study" /><category term="math" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[recording_a Summary of math]]></summary></entry><entry><title type="html">cross entropy</title><link href="https://chojinie.github.io/blog/2023/cs231-cross-entropy/" rel="alternate" type="text/html" title="cross entropy" /><published>2023-05-30T00:00:00+00:00</published><updated>2023-05-30T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/cs231-cross-entropy</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/cs231-cross-entropy/"><![CDATA[<h2 id="entropy엔트로피">entropy(엔트로피)</h2>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/entropy.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/entropy.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/entropy.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/entropy.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>엔트로피는 무질서, 임의성 또는 불확실성의 상태와 가장 일반적으로 관련된 과학적 개념이자 측정 가능한 물리적 특성입니다.(wikipedia) 고등학교, 대학교에 와서까지도 나오는 용어였습니다.
이제는 정보이론 측면에서 살펴보려고 합니다. 불확실성, 무질서의 상태를 측정하기 때문에 엔트로피 값이 클 수록 데이터가 분류가 잘 되어 있지 않은 상태이고 엔트로피 값이 작을 수록 데이터가 잘 분류되어 있는 것입니다.</p>

<h3 id="entropy-공식-log_2-는-lg2로-간략히">entropy 공식 (\(log_2\) 는 \(lg2\)로 간략히.)</h3>
<p>엔트로피 공식은 n개의 가능한 사건으로 일반화될 수 있습니다.
어떤 사건 A가 발생할 확률을 \(p\) 라고 할 때 엔트로피 공식은 아래와 같습니다.
\begin{equation} h(x) = -\sum_{i=1}^n (p_i lg_2(p_i)) \end{equation}
\(lg_2(p)\)에 \(p\)를 곱한 후 모두 더한 값에 -(minus)를 취해주는 것입니다.</p>

<p>이 식은 어떠한 개념에서 나오게 된 것일까요?<br /></p>

<p>엔트로피는 특성 사건에 대해 알기 위해 얻어야 하는 정보의 평균 비트 수입니다. 사건의 결과를 알기 위해서는 불확실성을 0으로 줄여야 합니다(즉, 확실성을 1로). 어떤 사건 A가 발생할 확률이 p라면 그 결과를 안다는 것은 불확실성을 \(\frac{1}{p}\) 만큼 줄이는 것을 의미합니다. 따라서 사건 결과에 대해 알기 위해서는 \(log\frac{1}{p}\) 비트 수가 필요하며 이는 \(-lg(p)\)와 같습니다. 이것은 A가 발생할 때의 엔트로피 값입니다. 마찬가지로 A가 일어나지 않을 때의 엔트로피 값은 \(-log(1-p)\)입니다. 확률 분포가 p를 갖는 <em>Bernoulli</em> 인 경우 사건의 평균 엔트로피는 \(-p * lg(p) -(1-p) * lg(1-p)\)입니다.</p>

<h3 id="엔트로피-vs-사건의-개수">엔트로피 VS 사건의 개수</h3>
<p>사건의 개수가 많아진다면, 엔트로피는 어떻게 될까요? 아마 높아질 것으로 직감할 수 있을 것입니다. 확률이 같은 n개의 이벤트를 선택하고 확률 분포의 엔트로피를 계산해보겠습니다.
\begin{equation} -n * \frac{1}{n} * lg\frac{1}{n} = lg(n) \end{equation}
n이 증가할 수록 log함수적으로 엔트로피는 증가하게 됩니다.</p>

<h2 id="cross-entropy교차-엔트로피-크로스-엔트로피">cross entropy(교차 엔트로피, 크로스 엔트로피)</h2>

<p>사건의 결과에 대해 알아야 하는 평균 비트 수는 정보를 전송하는 데 사용되는 평균 비트 수와 다릅니다. 교차 엔트로피는 정보를 전송하는 데 사용되는 평균 비트 수입니다. 교차 엔트로피는 항상 엔트로피보다 크거나 같습니다.
0.5, 0.25, 0.125 및 0.125의 확률로 네 가지 가능한 결과가 있는 확률 분포를 살펴보겠습니다. 이 정보를 전송하기 위해 2비트를 사용하면 교차 엔트로피는 2가 됩니다. 잠깐, 이 경우 엔트로피는 어떻게 계산될까요?</p>

<p>(1) 식에 대입하면, Entropy = 0.5 * lg(2) + 0.25 * lg(4) + 0.125 * lg(8) + 0.125 * lg(8)
= 0.5 + 0.5 + 0.375 + 0.375 = 1.75이 도출됩니다. 즉 여기서 엔트로피는 1.75입니다. 하지만, 정보의 전송을 위해 2bits를 사용했습니다. 즉 크로스 엔트로피는 2입니다. 이렇게 크로스 엔트로피와 엔트로피 간의 차이를 <strong><em>KL Divergence</em></strong>라고 합니다.</p>

<p>모든 경우에 대한 정보를 전송하기 위해 2비트를 사용할 때 모든 이벤트에 대해 \(\frac{1}{2^2}\)의 확률을 가정합니다. 따라서 실제 대 예측(또는 가정) 확률 분포는 다음과 같습니다. 0.5 vs 0.25, 0.25 vs 0.25, 0.125 vs 0.25 및 0.125 vs 0.25</p>

<p>첫 번째 이벤트 정보(1)를 전송하는 데 1비트를 사용하고 두 번째 이벤트 정보(10)를 전송하는 데 2비트를 사용하고, 세 번째 이벤트 정보(101)를 전송하는데 3비트, 네 번째 이벤트 정보(100)를 전송하는데 3비트를 사용했다면 최적의 메시지 길이를 사용했을 것입니다. 따라서 서로 다른 이벤트에 대해 서로 다른 메시지 길이를 사용하는 경우 암묵적으로 확률 분포를 예측합니다. 더 큰 교차 엔트로피 값을 얻을수록 예측 확률 분포가 실제 확률 분포에서 더 많이 벗어납니다. 그것이 분류를 다룰 때 기계 학습에서 교차 엔트로피 손실이 사용되는 방식입니다.</p>

<h3 id="cross-entropy">cross entropy</h3>
<p>참고로 크로스 엔트로피는 아래와 같이 수식으로 표현할 수 있습니다.
\begin{equation} h(p,q) = -\sum_{i=1}^n (q_i lg_2(p_i)) \end{equation}</p>

<p>h(p,q)는 오차를 의미합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic76-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic76-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic76-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic76.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>예측된 확률값(p)들이 [0, 0, 1] , [0, 1, 0]과 같이 완전히 틀리게 되면 lg(0)이 되므로 크로스 엔트로피는 무한대로 커진다.</p>

<h2 id="고양이-개-분류-예시">고양이 개 분류 예시</h2>

<p>가장 유명한 분류 문제 중 하나인 고양이와 개를 분류 문제에서는 이미지가 주어졌을 때 확률 분포를 예측하는 것입니다. 실제 확률 분포는 레이블이고 로지스틱 함수는 예측된 확률 분포를 제공합니다. 교차 엔트로피는 다음과 같은 방식으로 이 작업의 손실 함수로 사용됩니다.</p>

<p>개 이미지가 있고 확률 분포의 첫 번째 요소가 개에 대한 것이라고 가정합니다.
그렇다면 실제 확률 분포는 다음과 같습니다. : [1, 0]
이제 로지스틱 함수가 [0.7, 0.3]의 확률 분포를 출력했다고 가정합니다.</p>

<p>여기서 확률 0.7은 이미지가 개일 때 정보를 전송하는 데 사용되는 \(-lg(0.7)\) 비트 수를 의미합니다.
따라서 교차 엔트로피는 \(-1 * lg(0.7) - 0 * lg(0.3) = 0.51\)입니다.</p>

<p>어떤 함수를 최소화하는 것은 그것의 양의 스칼라 배수를 최소화하는 것과 같기 때문에 기계 학습에서 손실을 정의하기 위해 2-base 로그 대신 자연 로그(또는 임의의 기본 로그)를 사용할 수 있습니다.
이렇게 될 경우, lg를 쓸 때 보다 더 작은 값이 나오겠죠.</p>

<h2 id="참고">참고</h2>

<p>https://squarecircle.be/entropy-and-disorder-the-fate-of-all-human-enterprises/<br />
https://www.philgineer.com/2021/10/31.html<br />
https://westshine-data-analysis.tistory.com/<br />
https://www.youtube.com/watch?v=r3iRRQ2ViQM<br />
특히 수식을 이해하는 데에는 아래 사이트가 도움 되었습니다.
<strong>https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-17138ffab87b</strong></p>]]></content><author><name></name></author><category term="study" /><category term="cs231" /><category term="AI" /><category term="cs231" /><category term="study" /><category term="AI" /><summary type="html"><![CDATA[recording_a Summary of study]]></summary></entry><entry><title type="html">cs231n Summary - Introduction</title><link href="https://chojinie.github.io/blog/2023/cs231-Introduction/" rel="alternate" type="text/html" title="cs231n Summary - Introduction" /><published>2023-05-28T00:00:00+00:00</published><updated>2023-05-28T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/cs231-Introduction</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/cs231-Introduction/"><![CDATA[<p>본 시리즈는 cs231n을 공부하고 그 내용을 정리하기 위해 작성됩니다. 시리즈에 관련된 모든 출처는 기본적으로 http://cs231n.stanford.edu/schedule.html 의 강의 내용을 기반으로 하고 있습니다.</p>

<h3 id="강의-목표">강의 목표</h3>

<p>아래 이미지에서 교집합 부분에 해당하는 부분을 학습합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    
</div>

<h3 id="agenda">Agenda</h3>
<h4 id="컴퓨터-비전과-딥러닝의-기원의-요약">컴퓨터 비전과 딥러닝의 기원의 요약</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>카메라와 컴퓨터가 개발되면서, 어떻게 하면 컴퓨터가 사람과 같이 vision을 가질 수 있을지 연구하게 되었습니다.
이를 Computer Vision 분야라고 합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p>Hubel and Wiesel, 1959의 연구를 보면 생명체가 사물을 인식할 때 패턴에 따라 특정한 신호가 나오는 것을
알게 되었고 이를 컴퓨터에 적용하면 어떻게 될까에서 시작한 것 같습니다. 고전적인 Computer Vision
분야에서는 특징점을 찾아내서 사물을 구분하기도 했습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic8-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic8-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic8-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p>사람과 물체를 인식할 때 부분부분별로 따로 떼어서 인식하기도 했습니다. 또한, 인간이 사물을 파악할 때 edge(가장자리)로 구분 짓는다는 특징을 이용하여 edge detection 분야가 연구되기도 했습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic8-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic8-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic8-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h4 id="cs231n-overview">cs231n Overview</h4>

<h5 id="deep-learning-basics---image-classification">Deep Learning Basics - Image Classification</h5>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic35-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic35-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic35-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic35.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p>이미지 상의 대상을 무엇으로 분류할지를 classification task라고 합니다. 다양한 방법으로 이를 구현할 수 있습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic36-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic36-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic36-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic36.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic37-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic37-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic37-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic37.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic38-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic38-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic38-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic38.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>선분의 좌측이면 고양이 우측이면 강아지와 같은 방식으로 Classifier을 구현할 수 있습니다. 모델의 정확성을 높이기 위하여 Regularization과 Optimization을 수행하기도 합니다. Neural Network방식의 Classifier도 존재합니다.</p>

<h5 id="perceiving-and-understanding-the-visual-world">Perceiving and Understanding the Visual World</h5>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic39-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic39-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic39-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic39.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p>컴퓨터가 사물을 분류하는 등 역할을 수행하기 위해서는 결국 사물을 “인지”하고 “이해”하는 과정을 거쳐야 합니다.
사람에게는 정말 쉬운 작업이지만 컴퓨터에게는 이를 위해 다양한 task를 수행해야하며 여기에 맞는 모델이 필요합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic40-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic40-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic40-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic40.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic41-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic41-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic41-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic41.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic42-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic42-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic42-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic42.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic43-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic43-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic43-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic43.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h2 id="참고">참고</h2>
<p>http://cs231n.stanford.edu/schedule.html</p>]]></content><author><name></name></author><category term="study" /><category term="cs231" /><category term="AI" /><category term="cs231" /><category term="study" /><category term="AI" /><summary type="html"><![CDATA[recording_a Summary of lecture]]></summary></entry><entry><title type="html">cs231n Summary - Image Classification with Linear Classifiers</title><link href="https://chojinie.github.io/blog/2023/cs231-image-classification-linear/" rel="alternate" type="text/html" title="cs231n Summary - Image Classification with Linear Classifiers" /><published>2023-05-28T00:00:00+00:00</published><updated>2023-05-28T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/cs231-image-classification-linear</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/cs231-image-classification-linear/"><![CDATA[<h2 id="image-classification">Image Classification</h2>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic44-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic44-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic44-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic44.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic45-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic45-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic45-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic45.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>    
</div>

<p>컴퓨터가 고양이 사진을 본다면 밝기 성분을 [0, 255] 범위의 정수의 집합으로 표현할 것입니다. 예시의 고양이 사진의 전체사이즈는 대략 가로 세로 1:2 비율이라고 본다면, 200px X 400px크기의 이미지로 둘 수 있습니다. 또한 color이미지이므로 RGB 3channels로 구성되어 있습니다. 네모난 창문(window)을 보면 왼쪽 상단에서 우측 상단으로 탐색하면서 해당 위치는 RGB가 각각 얼마만큼 (0~255 사이) 성분이 있는지를 나타내는 값으로 저장됩니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/rgb-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/rgb-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/rgb-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/rgb.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>하지만, 만약 고양이 사진을 다른 각도에서 찍거나 조명이 달라지거나 한다면 밝기 성분은 당연히 달라질 수 밖에 없겠죠. 사람은 같은 고양이로 인식할 것입니다. 누워있던 숨어있던 뒤돌아 있던, 하지만 컴퓨터는 어렵죠.<br /></p>

<p>Challenges of recognition :</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic49-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic49-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic49-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic49.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>물론 고전적인 Computer vision에서는 edge detection 등 많은 방식으로 Classification task(분류 문제)를 풀려고 했습니다. 하지만 이렇다할 성능을 보이지는 못했고, 현대에 와서는 Data-driven 방식의 머신러닝 기법을 적용하게 되었습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic53-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic53-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic53-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic53.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic54-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic54-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic54-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic54.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h3 id="nearest-neighbor-classifier">Nearest Neighbor Classifier</h3>

<p>KNN(K-Nearest Neighbor)분류기입니다. 우리가 타겟하는 포인트와 가까이에 있는 k개를 살펴보고 k개의 포인트가 가장 많이 속해있는 집단(class)을 정답으로 하는 분류기입니다. 
사진에서 고양이를 찾는다고 다시 생각해보겠습니다. 아래 그림처럼 어느 한 point가 우리가 찾고자하는 지점이라고 하겠습니다. k = 3으로 둘 경우, 타겟 포인트로 부터 근처에 3개의 샘플을 추출하여 어느 영역에 더 많이 속해있는지를 보고 타겟 포인트도 해당 영역의 값이라고 예측하는 것입니다.
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic58-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic58-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic58-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic58.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>


</p>
<p>task 수행을 위해 모델이 필요하다고 했었죠. 하지만 KNN의 경우 특별한 모델이 필요한 것은 아니고 training data(학습 데이터)의 정답을 모두 저장해두었다가 나중에 test data(테스트 데이터)를 모델에 넣었을 때 정답과 가장 유사한 결과를 예측해내어 분류 task를 수행하는 것이 전부입니다. 그렇기 때문에 복잡한 인공지능 여타 모델과 달리 lazy model이라고 이야기 하기도 합니다.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic55-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic55-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic55-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic55.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>고양이라는 (Label)을 달고 있는 training data를 저장해두었다가, 나중에 query로 들어오는 Test data와의 거리 비교(Distance Metric)를 하여 그 차이가 가장 작은 데이터를 고양이라고 분류하는 거죠.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic56-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic56-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic56-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic56.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>Distance Metric 즉, 거리 차이를 비교하는 방법은 대표적으로 두가지가 있습니다. L1 distance와 L2 distance 입니다.</p>

<p>L1 distance: 
\begin{equation} 
d_1(I_1, I_2) = \sum\limits_{p}|I_1^{p} - I_2^{p}| 
\end{equation}</p>

<p>두 이미지를 L1 거리로 비교하기 위해 픽셀별 차이를 사용하는 예입니다(예에서는 하나의 색상 채널에 대해).
두 개의 이미지를 요소별로 뺀 다음 모든 차이를 더하여 하나의 숫자로 만듭니다. 두 이미지가 동일하면 결과는 0이 됩니다. 그러나 이미지가 매우 다르면 결과가 커집니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic57-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic57-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic57-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic57.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>L2 distance: \begin{equation} d_1(I_1, I_2) = \sqrt{\sum\limits_{p}(I_1^{p} - I_2^{p})^2} \end{equation}</p>

<p><strong>L1 VS L2</strong>  특히, L2 거리는 L1 거리보다 두 벡터 사이의 차이에 대해 훨씬 더 까다롭습니다. 즉, L2 거리는 하나의 큰 불일치보다 많은 중간 불일치를 선호합니다.</p>

<h4 id="hyperparameter-setting">Hyperparameter setting</h4>
<p>그렇다면, 얼마만큼의 k를 줘야 최적의 답을 끌어내는 모델일까요? 혹은 어떠한 distance metric을 사용해야 최고의 모델이 될까요?<br />
이러한 요소들이 우리들이 학습하면서 수정해 나아가야 할 KNN모델의 Hyperparameter라고 합니다.
매우 어려운 부분이고, 데이터셋에 의존적입니다. 모든 알고리즘을 실행해보고 어떤 것이 가장 잘 동작하는지 확인을 해야합니다.
</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic59-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic59-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic59-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic59.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>여러 가지 다른 값들을 시도해보고 가장 좋은 결과를 찾는 것은 당연히 좋은 아이디어이며, 실제로 수행해야 할 일입니다. 하지만 이 매우 신중하게 수행되어야 합니다. 특히 <em>하이퍼파라미터를 조정하기 위해 테스트 세트를 사용할 수 없습니다.</em> 기계 학습 알고리즘을 설계할 때 테스트 세트는 매우 귀중한 자원으로 간주해야 하며, 가능하면 맨 마지막에 한 번만 사용하는 것이 이상적입니다. 그렇지 않으면 실제로 모델을 배포할 때 성능이 크게 저하될 수 있는 실질적인 위험이 있습니다. 실전에서는 테스트 세트에 과적합(overfit)된다고 말합니다.</p>

<p>다행히도 하이퍼파라미터를 조정하는 올바른 방법이 있으며, 이는 테스트 세트를 전혀 건드리지 않습니다. 아이디어는 훈련 세트를 두 개로 나누는 것입니다: 조금 더 작은 훈련 세트와 우리가 검증 세트라고 부르는 세트입니다. CIFAR-10을 예로 들면, 예를 들어 49,000개의 훈련 이미지를 훈련용으로 사용하고, 나머지 1,000개를 검증용으로 남겨둘 수 있습니다. 이 검증 세트는 사실상 하이퍼파라미터를 조정하기 위한 가짜 테스트 세트로 사용됩니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic60-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic60-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic60-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic60.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>훈련 데이터의 크기(따라서 검증 데이터의 크기도)가 작을 수 있는 경우, 하이퍼파라미터 튜닝을 위해 교차 검증(cross-validation)이라는 더 정교한 기법을 사용하는 경우도 있습니다. 이전의 예제를 가지고 작업할 때, 임의로 처음 1,000개의 데이터 포인트를 검증 세트로 선택하는 대신, 교차 검증을 사용하여 특정 k 값의 성능을 더 좋고 노이즈가 적게 평가할 수 있습니다. 예를 들어, 5-fold 교차 검증에서는 훈련 데이터를 5개의 동일한 폴드로 분할하여 4개를 훈련에 사용하고 1개를 검증에 사용합니다. 그런 다음 어떤 폴드가 검증 폴드인지 반복하면서 성능을 평가하고, 마지막으로 다른 폴드들 사이에서 성능을 평균합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic61-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic61-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic61-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic61.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h4 id="k-nearest-neighbor-with-pixel-distance-never-used">k-Nearest Neighbor with pixel distance never used</h4>

<p>k-Nearest Neighbor 알고리즘에서 픽셀 간의 거리를 사용하지 않는 이유는 다음과 같습니다:</p>

<ol>
  <li>차원의 저주 (Curse of Dimensionality): 이미지 데이터의 경우 각 픽셀은 하나의 차원으로 간주됩니다. 이미지의 크기가 커질수록 차원의 수가 급격히 증가하게 되는데, 이는 차원의 저주로 알려진 문제를 야기할 수 있습니다. 차원의 저주는 고차원 공간에서 데이터 간의 거리 측정이 더 어렵고, 더 많은 데이터가 필요하게 만들어서 k-최근접 이웃 알고리즘의 성능을 저하시킵니다. \(2^{3000}\) 보기만 해도 계산하기엔 참 어렵겠죠.</li>
  <li>픽셀 간의 거리는 이미지의 의미를 적절하게 반영하지 않을 수 있습니다. 이미지는 일반적으로 시각적인 패턴, 구조, 텍스처 등을 포함하고 있으며, 이러한 특징들은 단순히 픽셀 값의 거리로만 표현하기에는 제한적일 수 있습니다. 따라서 픽셀 간의 거리를 사용하지 않는 대안적인 특징 추출 방법이나 거리 측정 방법을 사용하는 것이 더 효과적일 수 있습니다.</li>
</ol>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic62-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic62-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic62-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic62.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>왼쪽에는 원본 이미지가 있고 그 옆에는 픽셀 기반 L2 거리에 따라 모두 동일한 거리에 있는 세 개의 다른 이미지가 있습니다. 분명히, 픽셀 단위의 거리는 인지적이거나 의미론적인 유사성과 전혀 일치하지 않습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic63-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic63-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic63-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic63.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>따라서, 이미지 데이터의 경우에는 픽셀 간의 거리를 직접 사용하지 않고, 보다 효과적인 특징 추출 방법과 Distance Metric(L1, L2, etc) 방법을 활용하는 것이 일반적입니다.</p>

<h2 id="linear-classifier">Linear Classifier</h2>
<p>선형 분류기(Linear Classifier)는 입력 데이터를 선형 경계로 구분하는 분류 알고리즘입니다. 주어진 입력 데이터에 대해 각각의 특성을 가중치와 결합하여 선형 함수를 생성하고, 이 함수의 결과를 기반으로 데이터를 클래스로 분류합니다. 이 분류방식을 잘 학습해야 Neural Network와 Convolutional Nueral Network로 자연스럽게 개념을 확장해 갈 수 있습니다.</p>

<p>이 기법에는 원본 데이터를 클래스 점수에 맵핑하는 점수 함수(score function)와, 예측된 점수들과 참값 레이블이 얼마나 일치하는지를 정량화한 손실 함수(loss function), 이렇게 두 가지 중요한 요소가 있습니다. 이들을 이용하면 이미지 분류 문제를 점수 함수의 파라미터에 대해 손실 함수를 최소화하는 최적화 문제(optimization problem)로 바꿀 수 있습니다.</p>

<h3 id="이미지에서-레이블의-점수로의-parameterized-mapping">이미지에서 레이블의 점수로의 Parameterized mapping</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic64-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic64-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic64-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic64.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>이 접근 방식의 첫 번째 구성 요소는 이미지의 픽셀 값을 각 클래스의 신뢰도 점수에 매핑하는 점수 함수를 정의하는 것입니다. 구체적인 예를 들어서 접근 방식을 가시화 시켜보겠습니다. 그전에, 이미지의 training dataset인 \(x_i \in \mathbf{R}^D\) 의 성분을 정해 놓고 이야기 해보겠습니다. 각각의 이미지들은 \(y_i (i = 1 \dots N)\)라는 레이블들과 엮여 있습니다. 즉 \(x_i\) 이미지는 \(y_i\)라는 레이블을 갖게 되는 것입니다. 그리고 \(y_i \in 1 \dots K\)입니다. 즉, N개의 example이 있으며(Example 각각 D 차원) K개의 별개의 카테고리가 있다는 의미입니다.</p>

<p>예를 한 번 들어보죠
CIFAAR-10 데이터셋에서 training data는 50,000장이 있습니다. (N=50,000) 이미지는 각각 D차원 (D = 32 X 32 X 3 = 3072pixel) 로 되어 있고, K=10이기 때문에 10개의 카테고리(Classes dog, cat, car and so on and so forth)로 분류가 되는 것입니다. 이제 점수 함수(score function)는 $$ \mathbf{f: R^D \rightarrow R^K} $$ 로 정의할 수 있으며, raw 이미지 픽셀을 클래스 점수에 매핑할 수 있게 됩니다.
</p>

<p>


**Linear classifier.**  가장 간단한 함수인 선형 함수의 매핑 형태입니다.


\begin{equation} f(x_i, W, b) = Wx_i + b \end{equation}
</p>

<p>위의 수식에서 이미지 \(x_i\)는 모든 픽셀을 D X 1 형태의 하나의 열 벡터로 <strong>Flatten</strong>시킨다고 가정하겠습니다. 행렬 W(size : [K X D])와 벡터 b(size : [K X 1])는 함수의 매개변수(parameter)입니다. CIFAR-10에서 \(x_i\)는 단일 [3072 x 1] 열 벡터로 병합된 i 번째 이미지의 모든 픽셀을 포함하고 있습니다. W는 [10 X 3072]이고 b는 [10 X 1] 크기 입니다. 따라서 3072개의 숫자가 input으로 들어오고(raw image pixels) 10개의 숫자가 output으로 나오게 됩니다.(클래스 점수)</p>

<p><strong>w</strong>는 Weight(가중치)라고 합니다. b는 bias vector(편향 벡터)라고 하고, 실제 데이터 \(x_i\)와 상호 작용하지 않고 출력 점수 값에 영향을 주게 됩니다. 입력 특성들이 모두 0인 경우에도 분류기가 적절한 예측을 할 수 있도록 해줍니다. 즉, 바이어스는 결정 경계의 위치를 조정하여 데이터를 잘 분리할 수 있도록 도와주는 역할을 합니다. W와 b가 parameter에 속하지만 보통은 사람들은 가중치와 매개변수라는 용어를 같은 의미로 사용하기도 합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic65-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic65-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic65-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic65.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h3 id="linear-classifier의-해석-방법">Linear classifier의 해석 방법</h3>

<p>선형 분류기는 3개의 색상 채널 모두에서 모든 픽셀 값의 가중 합계로 클래스의 점수를 계산합니다. 이러한 가중치에 대해 정확히 어떤 값을 설정했는지에 따라 함수는 이미지의 특정 위치에서 특정 색상을 좋아하거나 싫어할 수 있습니다(각 가중치의 부호에 따라). 예를 들어, 이미지 측면에 파란색이 많이 있는 경우(물에 해당할 수 있음) “선박” 클래스일 가능성이 더 높다고 상상할 수 있습니다. 그러면 “선박” 분류기가 파란색 채널 가중치에서 많은 양의 가중치를 가질 것이라고 예상할 수 있습니다. (파란색의 존재는 배의 점수를 높임), 빨강/녹색 채널의 음수 가중치(빨간색/녹색의 존재는 배의 점수를 감소시킵니다).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic66-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic66-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic66-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic66.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>위 그림을 설명해보겠습니다. 시각화를 위해 이미지에 4개의 픽셀(단색 픽셀 4개, 간결함을 위해 이 예에서는 색상 채널을 고려하지 않음)만 있고 3개의 클래스(빨간색(고양이), 녹색(개), 파란색(배))가 있다고 가정합니다. (설명: 특히 여기서 색상은 단순히 3개의 클래스를 나타내며 RGB 채널과 관련이 없습니다.) 이미지 픽셀을 열로 쭉 펴고 행렬 곱셈을 수행하여 각 클래스의 점수를 얻습니다. 이 특정 가중치 세트 W는 전혀 좋지 않습니다. 가중치는 고양이 이미지에 매우 낮은 고양이 점수를 할당합니다. 특히, 이 가중치 세트는 개를 보고 있다고 확신하는 것 같습니다.</p>

<h4 id="고차원-점으로-이미지를-유추analogy-of-images-as-high-dimensional-points"><strong>고차원 점으로 이미지를 유추(Analogy of images as high-dimensional points.)</strong></h4>

<p>이미지가 고차원 열 벡터로 확장되기 때문에 각 이미지를 이 공간의 단일 지점으로 해석할 수 있습니다(예: CIFAR-10의 각 이미지는 32x32x3 픽셀의 3072차원 공간의 지점입니다). 마찬가지로 전체 데이터 세트는 (레이블이 지정된) 포인트 세트입니다. 각 클래스의 점수를 모든 이미지 픽셀의 가중 합으로 정의했으므로 각 클래스 점수는 이 공간에 대한 선형 함수입니다. 3072차원 공간을 시각화할 수는 없지만 모든 차원을 2차원으로 압축한다고 상상하면 분류기가 수행할 수 있는 작업을 시각화할 수 있습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic68-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic68-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic68-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic68.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>이미지 공간을 표현한 이미지. 데이터 셋의 각 이미지들은 하나의 점으로, 선형 분류기는 선으로 표현되어 있습니다. 자동차 분류기(빨간색)의 예를 사용하여 빨간색 선은 자동차 클래스에 대해 0점을 받은 공간의 모든 지점을 보여줍니다. 이때 빨간 화살표는 클래스 점수가 증가하는 방향을 보여줍니다. 즉, 빨간 선 오른편의 모든 점들은 (화살표 방향으로 선형적으로 증가하는) 양수 점수를 가지고, 빨간 선 왼편의 모든 점들은 (화살표 방향으로 선형적으로 감소하는) 음수 점수를 가집니다.<br /></p>

<p>위에서 보았듯이 <strong>W</strong> 의 모든 행 벡터는 클래스 중 하나에 대한 분류기가 됩니다. 이 숫자의 기하학적 해석은 W의 행 중 하나를 변경할 때, 픽셀 공간의 해당 라인은 다른 방향으로 회전합니다. 반면에 Biase인 b는 분류기가 수평이동 할 수 있도록 합니다. 특히 bias term이 없다면, \(x_i=0\) 일 경우, 가중치에 관계없이 항상 0점을 주므로 모든 선이 원점을 지나도록 강제됩니다.</p>

<h4 id="선형-분류기를-템플릿-매칭으로-해석interpretation-of-linear-classifiers-as-template-matching"><strong>선형 분류기를 템플릿 매칭으로 해석(Interpretation of linear classifiers as template matching)</strong></h4>

<p>가중치 <strong>W</strong> 에 대한 또 다른 해석은 W의 각 행은 클래스 중 하나에 대한 템플릿(또는 때로는 프로토타입이라고도 함)에 해당한다고 보는 것입니다. 그런 다음 이미지에 대한 각 클래스의 점수는 이미지와 각 템플릿을 하나씩 내적하여 가장 “적합한 템플릿”을 찾는 방식으로 얻습니다. 다르게 생각해보면 즉, 선형 분류기는 여전히 Nearest Neighbor를 찾는 방식으로 효과적으로 수행하고 있지만 수천 개의 훈련 이미지를 다 가지고 있는 것이 아니라 클래스당 하나의 템플릿만 사용하여, L1 또는 L2 거리 대신 (음의) 내적을 거리로 사용합니다. 선형 분류기가 가지고 있는 템플릿 이미지는 학습을 통해 만들어지는데, 템플릿 이미지가 반드시 학습 데이터의 사진 중 한 장일 필요는 없습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic67-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic67-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic67-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic67.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>예를 들어 ship 템플릿에는 예상대로 파란색 픽셀이 많이 포함되어 있습니다. 따라서 이 템플릿은 해상의 배 이미지와 내적을 통해 일치되었을 때 높은 점수를 줄 것입니다.</p>

<h2 id="좋은-가중치weight-선택하기---loss-function">좋은 가중치(Weight) 선택하기 - Loss function</h2>

<p>앞선 고양이 이미지 분류 문제에서 -96.8이라는 저조한 성적을 거둔 것을 알 수 있었습니다. 이제는 손실함수(때때로 cost function or objective라고 함)를 통해 훈련 데이터의 점수에 대한 불만을 측정하는 손실 함수를 정의하겠습니다. 직관적으로 보면, 훈련 데이터 분류를 잘 수행하면 손실이 낮을 것이고 반대면 높겠죠.<br /></p>

<h3 id="multiclass-support-vector-machine-loss">Multiclass Support Vector Machine loss</h3>
<p>SVM(Multiclass Support Vector Machine) 손실 함수를 살펴보겠습니다. SVM 손실은 SVM이 각 이미지에 대한 올바른 클래스가 고정된 margin \(\Delta\)만큼 잘못된 클래스보다 높은 점수를 갖기를 “원”하도록 설정됩니다. 손실 함수를 의인화하여 능동적으로 “원한다”고 표현하는 것은 종종 이해에 도움이 됩니다. SVM은 결과가 더 낮은 손실(좋음)을 산출한다는 의미에서 특정 결과를 “원합니다”.</p>

<p>수식으로 표현해보겠습니다. \(i\) 번째 이미지의 전체 픽셀을 하나의 열 벡터로 만든 \(x_i\) 와 해당 미지의 label \(y_i\) 가 있을 경우, score function(f)는 \(x_i\)를 입력받아 클래스 점수를 나타내는 벡터 \(s=f(x_i, W)\)를 출력합니다.(s는 점수 score를 의미) \(x_i\) 의 \(j\) 번째 클래스의 점수는 \(s_j\)의 \(j\)번째 요소인 \(s_j = f(x_i, W)_{j}\)라 표현할 수 있습니다.</p>

<p>\(i\)번째 이미지에 대한 SVM 손실 함수는 다음과 같이 일반화할 수 있습니다. \(max(0, ~)\) 형태는 힌지 손실 함수(hinge loss)라 불리는, 0에 대한 문턱값(threshold)을 나타내는 함수입니다.</p>

<p>\begin{equation} L_i = \sum\limits_{j\neq y_i}max(0, s_j - s_{y_i} + \mathit{\Delta}) \end{equation}</p>

<h4 id="예제"><strong>예제</strong></h4>
<p>예시를 들어 자세히 설명해 보겠습니다. 세개의 클래스가 각각 \(s = [13, -7, 11]\) 만큼의 점수를 가지고 있고 첫 번째 클래스가 정답(true class, \(i.e. y_i = 0\))이라고 가정해보겠습니다. 그리고 \(\Delta = 10\) 으로 두겠습니다. 위 식은 잘못 매칭된 클래스 들의 점수의 합계라고 볼 수 있죠. (\(j \neq y_i\)) 이제 식에 대입해보겠습니다.</p>

<p>\begin{equation} L_i = max(0, -7-13+10) + max(0, 11-13+10) \end{equation}</p>

<p>첫 번째 항은 손실이 0이 됩니다. 올바른 클래스 점수(13)가 잘못된 클래스 점수(-7)보다 최소 마진 10만큼 크기 때문에 이 쌍에 대해 손실이 0이 됩니다. 두 번째 항은 손실이 8이 됩니다. 요약하면 SVM 손실 함수는 올바른 클래스 yi의 점수를 원합니다. 잘못된 클래스 점수보다 적어도 \(\Delta\) 만큼 더 커야 합니다. 점수 함수를 선형 함수\((f(x_i;W)=Wx_i)\)로 작업했었죠. 그렇다면 (4)번 식은 이렇게 바꿔 쓸 수도 있습니다. 
\begin{equation} L_i = \sum_{j \neq y_i}max(0, \omega_{j}^{T}x_i - \omega_{y_i}^{T}x_i + \Delta)\end{equation}</p>

<p>\(\omega_{j}\)는 \(W\)의 j번째 행이 열로 재구성된 것입니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic73-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic73-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic73-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic73.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p>\(\Delta\) 보다 작은 항을 더 강하게 불리하게 만들기 위해 선형적인 식이 아닌 2차식 형태의 제곱 힌지 손실 SVM 함수(squared hinge loss SVM, 혹은 \(L_2-SVM, max(0,−)^2\))을 대신 사용할 때도 있습니다. 제곱하지 않은 단순한 힌지 손실 함수를 사용하는 것이 더 일반적인 형태이나, 몇몇 데이터 셋에서는 제곱 힌지 손실 함수가 더 잘 작동할 수 있습니다. 두 손실값 중 어느쪽이 더 나은지는 교차 검증법(cross validation)을 적용해 결정합니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic74-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic74-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic74-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic74.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>아래 예시를 보면 Multiclass SVM 손실 함수를 어떻게 구해야 할지 알 수 있습니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic69-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic69-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic69-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic69.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic70-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic70-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic70-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic70.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic71-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic71-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic71-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic71.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic72-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic72-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic72-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic72.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h2 id="softmax-clssifier">Softmax Clssifier</h2>

<p>소프트맥스 분류기(Softmax Classifier)는 다중 클래스 분류 문제를 해결하기 위한
알고리즘입니다.SVM이 출력으로 \(f(x_i, W)\) 를 각 클래스가 나올 점수(score)로 할당하는 
것과는 달리 Softmax classifier는 normalize된 클래스 확률로 더욱 직관적인 출력을 보여줍니다. 
함수 자체가 맵핑되는 것은 \(f(x_i, W)\)로 SVM과 같습니다. 하지만, 점수를 unnormalized 
log probabilties(비정규화 로그 확률)로 해석하고 힌지 손실을 교차 엔트로피 손실로 대체해서 
해석합니다.</p>

<p>\begin{equation} L_i = -log(\frac{e^{f_{y_i}}}{\sum_{j}e^{f_j}}) 
\end{equation}</p>

<p>또는</p>

<p>\begin{equation}
L_i = -f_{y_i}+ log\sum_{j}e^{f_j}
\end{equation}</p>

<p>\(f_j\)는 클래스 점수 f의 벡터의 j번째 요소를 의미합니다. \(f_j(Z) = \frac{e^{f_{y_i}}}{\sum_{j}e^{f_j}}\) 은 <strong>softmax function</strong> 입니다. 임의의 실수값으로 이루어진 벡터를 받아서(in Z), 해당 벡터를 0과 1 사이의 값으로 압축하고, 그 값들의 합이 1이 되도록 만듭니다. softmax 기능과 관련된 전체 교차 엔트로피 손실은 어려워 보일 수 있지만, 자주 쓰일 개념이므로 잘 이해해야 합니다. 해당 글은 <a href="https://chojinie.github.io/blog/2023/cs231-cross-entropy/">cross entropy</a>편에 정리해 놨습니다.</p>

<h3 id="정보이론-관점에서-해석">정보이론 관점에서 해석</h3>
<p>참 값의 분포인 p와 예측 분포인 q간의 관계는 아래와 같이 정의 할 수 있습니다.
\begin{equation} h(p,q) = -\sum_{i=1}^n (q_i lg_2(p_i)) \end{equation}</p>

<p>소프트맥스 분류기는 예측 클래스 확률 (q = \frac{e^{f_{y_i}}}{\sum_{j}e^{f_j}})와 참 값의 분포 사이의 크로스 엔트로피를 최소화 합니다. 이 해석에서 “참 값”의 분포는 모든 확률 질량이 정답 클래스에 위치한 분포입니다.(즉, p = [0, …, 1, …,0]은 \(y_i\) 번째 위치에 단일한 1을 갖습니다.) 또한 교차 엔트로피는 엔트로피와 KL Divergence의 합으로 나타낼 수 있습니다. 델타 함수 p의 엔트로피는 0이므로, 이는 두 분포 간의 KL Divergence를 최소화하는 것과 동등합니다. 즉, 교차 엔트로피 목적은 예측된 분포가 올바른 답에 대한 확률을 모두 가지도록 하는 것입니다.</p>

<h3 id="확률론-관점에서-해석">확률론 관점에서 해석</h3>

<p>이를 이해하기 위해, 소프트맥스 분류기는 출력 벡터 “f” 내의 점수를 비정규화된 로그 확률로 해석합니다. 
이러한 값을 지수화함으로써 (비정규화된) 확률을 얻을 수 있으며, 나눗셈은 확률을 정규화하여 확률이 1이 
되도록 합니다. 확률적 해석에서는 따라서 올바른 클래스의 음의 로그 우도를 최소화하는 것으로, 최대 우도 
추정(MLE)을 수행한다고 볼 수 있습니다.</p>

<!--예를 들어, 3개의 수 $$ x_0=2, x_1=1, x_2=-1 $$이 있고 이 수의 대소 관계를 유지하면서 각각의 -->
<!--확률을 나타내는 $$ y_0, y_1, y_2 $$로 변환하려고 할 경우, 확률이므로 0에서 1 사이의 숫자가 -->
<!--나와야 하며, 모두 더하면 1이어야만 합니다. 이 떄 사용되는 것이 소프트맥스 (Softmax) 함수입니다. -->
<!--먼저 $$ x_i $$의 지수함수(exp) 꼴들의 합계를 구해서 u라고 두겠습니다. 지수함수가 사용되는 이유는 -->
<!--미분이 가능하도록 하게 함이며, 입력값 중 큰 값은 더 크게 작은 값은 더 작게 만들어 입력벡터가 더 잘 -->
<!--구분되게 하기 위함입니다.-->
<!---->
<!--\begin{equation} u = exp(x_0) + exp(x_1) + exp(x_2)-->
<!--\end{equation}-->
<!--위 식을 사용하여 y꼴로 변환해준다면, $$ y_0 = \frac{exp(x_0)}{u} , y_1 = \frac{exp(x_1)}{u}, y_2 = \frac{exp(x_2)}{u} $$ 로 나타낼 수 있습니다. <br>-->

<p>소프트맥스 분류기는 손실 함수로 크로스 엔트로피 손실(cross-entropy loss)을 사용하여 모델을 
훈련합니다. 크로스 엔트로피 손실은 실제 클래스와 예측된 클래스의 확률 분포 사이의 차이를 측정합니다. 
모델의 매개변수를 업데이트하면서 손실을 최소화하는 방향으로 학습이 진행됩니다.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cs231n/assignment1/pic75-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cs231n/assignment1/pic75-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cs231n/assignment1/pic75-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cs231n/assignment1/pic75.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p>가장 왼쪽의 파란 박스는 logits이라고 합니다. 이는 확률화되지 않은 날 것의 예측 결과를 지칭합니다. 멀티 클래스 분류문제에서 보통 softmax 함수의 입력으로 사용됩니다. (4) 식의 소프트맥스 함수에 적용하여 exponential을 취해줍니다.
그 결과가 빨간색 박스 형태로 나오게 됩니다. 무조건 0 이상의 수가 나오기 때문에 확률로 변경할 수 있게 됩니다. 이후에  \(\sum\limits_{j}e^{s_j}\) 로 나눠서 normalize 시켜, 우리가 아는 분수 형태의 확률 값 형태로 변형시킵니다.
확률이므로 각 클래스 별로 나오게 될 숫자의 합은 무조건 “1”이 나와야만 합니다.</p>

<h3 id="softmax-분류기는-각-클래스에-대한-확률을-제공합니다">Softmax 분류기는 각 클래스에 대한 “확률”을 제공합니다.</h3>

<p>보정되지 않고 모든 클래스에 대한 점수를 해석하기 어려운 SVM과는 달리 소프트맥스 분류기는 모든 레이블에 
대한 확률을 계산할 수 있습니다. 예를 들어 이미지가 주어지면 SVM 분류기는 “cat”, “dog” 및 
“ship” 클래스에 대해 [12.5, 0.6, -23.0] 점수를 줄 수 있습니다. 대신 softmax 분류기는 세 
레이블의 “확률”을 [0.9, 0.09, 0.01]로 계산할 수 있습니다. 이렇게 각 클래스에 대한 신뢰도를 해석 
할 수 있게 됩니다. 그러나 “확률”이라는 단어에 강조를 하는 이유는 이러한 확률의 뾰족함 또는 흩어짐이 
정규화 강도 \lamda 에 직접적으로 의존하기 때문입니다. 이 정규화 강도는 시스템에 입력으로 주어지는 
값입니다. 예를 들어, 어떤 세 개의 클래스에 대한 비정규화된 로그 확률이 [1, -2, 0]으로 나온다고 
가정해보겠습니다. softmax 함수는 다음을 계산합니다. 
\begin{equation} [1,-2,0] \rightarrow [e^1,e^{-2},e^0]=[2.71,0.14,1] 
\rightarrow [0.7,0.04,0.26] \end{equation}
스코어를 exp로 지수화하고, 그 값을 정규화하여 확률의 합계를 1로 만드는 것입니다. (0.55 + 0.12 + 
0.33 = 1) 이제 확률은 더 흩어져 있습니다. 또한, 매우 강한 정규화 강도 λ로 인해 가중치가 매우 작은 
수로 수렴하는 한계에서는 출력 확률이 균일에 가까워질 것입니다. 따라서 소프트맥스 분류기에 의해 계산된 
확률은 SVM과 유사하게 순위 지정 가능하지만, 절대적인 숫자(또는 그 차이)는 기술적으로 해석할 수 없는 
대신, 신뢰도로 생각하는 것이 더 나은 접근입니다.</p>

<h3 id="실제로-svm과-softmax는-일반적으로-비슷합니다">실제로 SVM과 Softmax는 일반적으로 비슷합니다.</h3>

<p>SVM과 Softmax의 성능 차이는 일반적으로 매우 작으며, 어떤 분류기가 더 잘 작동하는지에 대한 의견은 
다양할 수 있습니다. Softmax 분류기와 비교하여 SVM은 더 지역적인 목적 함수로, 버그로 생각될 수도 
있고 특징으로 생각될 수도 있습니다. [10, -2, 3]의 점수를 달성하고 첫 번째 클래스가 올바른 예를 
고려해보겠습니다. SVM (예: 원하는 여유 간격 \Delta =1을 가진 경우)은 올바른 클래스가 다른 
클래스들과 비교하여 여유 간격보다 높은 점수를 이미 가지고 있으므로 손실을 0으로 계산합니다. SVM은 
개별 점수의 세부 사항에는 관심이 없습니다. 예를 들어, 점수가 [10, -100, -100]이거나 [10, 9, 
9]라면, SVM은 여유 간격이 1로 만족되므로 손실이 0이기 때문에 차이를 느끼지 않습니다. 그러나 이러한 
시나리오는 Softmax 분류기와 동일하지 않습니다. Softmax 분류기는 점수 [10, 9, 9]에 대해 [10, 
-100, -100]보다 훨씬 더 높은 손실을 누적합니다. 다시 말해, Softmax 분류기는 생성된 점수에 대해 
완전히 만족하지 않습니다. 올바른 클래스는 항상 더 높은 확률을 가지고 있어야 하고, 잘못된 클래스는 항상 
더 낮은 확률을 가져야 하며, 손실은 항상 좋아집니다. 그러나 SVM은 여유 간격이 충족되면 만족하며, 
정확한 점수를 이 제약 조건 이상으로 관리하지 않습니다. 이는 직관적으로 특징으로 생각할 수 있습니다. 
예를 들어, 자동차 분류기는 자동차와 트럭을 분리하는 어려운 문제에 대부분의 “노력”을 기울이고 있을 
것이며, 이미 매우 낮은 점수를 할당하는 개구리 예제에 영향을 받지 않아야 하며, 아마도 데이터 클라우드의 
완전히 다른 쪽에 군집화될 가능성이 높습니다.</p>

<h2 id="참고">참고</h2>
<p>https://heekangpark.github.io/Stanford_cs231n/03-linear-classification<br />
http://cs231n.stanford.edu/schedule.html</p>]]></content><author><name></name></author><category term="study" /><category term="cs231" /><category term="AI" /><category term="cs231" /><category term="study" /><category term="AI" /><summary type="html"><![CDATA[recording_a Summary of lecture]]></summary></entry><entry><title type="html">cs231n assignment solution</title><link href="https://chojinie.github.io/blog/2023/solution-cs231n-assignment/" rel="alternate" type="text/html" title="cs231n assignment solution" /><published>2023-05-27T00:00:00+00:00</published><updated>2023-05-27T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/solution-cs231n-assignment</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/solution-cs231n-assignment/"><![CDATA[<h2 id="assignment-1">Assignment 1</h2>
<h3 id="inline-question-1">Inline Question 1</h3>
<p>Notice the structured patterns in the distance matrix, where some rows or columns are visibly brighter. 
(Note that with the default color scheme black indicates low distances while white indicates high distances.)</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/assignmentinlinequestion1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/assignmentinlinequestion1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/assignmentinlinequestion1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/assignmentinlinequestion1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>What in the data is the cause behind the distinctly bright rows?</li>
  <li>What causes the columns?</li>
</ul>

<p>\(\color{blue}Your Answer :\) <em>fill this in.</em></p>

<p>
- 훈련 데이터 세트에 포함되지 않은 클래스로부터의 관측이거나, 적어도 대부분의 훈련 데이터와 매우 다른 관측일 가능성이 높습니다. 아마도 배경 색상과 관련하여 큰 차이가 있을 것입니다.<br />
- 학습 데이터 포인트가 테스트 데이터 내의 어떠한 포인트와도 비슷하지 않음을 의미합니다.
</p>]]></content><author><name></name></author><category term="study" /><category term="cs231" /><category term="AI" /><category term="cs231" /><category term="study" /><category term="AI" /><summary type="html"><![CDATA[recording_a Summary of assignment]]></summary></entry><entry><title type="html">Numpy study</title><link href="https://chojinie.github.io/blog/2023/numpy-study.md/" rel="alternate" type="text/html" title="Numpy study" /><published>2023-05-26T00:00:00+00:00</published><updated>2023-05-26T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/numpy-study.md</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/numpy-study.md/"><![CDATA[<h3 id="numpyrandomchoicea-sizenone-replacetrue-pnone">numpy.random.choice(a, size=None, replace=True, p=None)</h3>

<p>a = 1차원 배열 또는 정수 (정수인 경우, np.arrange(a)와 같은 배열 생성)<br />
size = 정수 또는 튜플(튜플인 경우, 행렬로 리턴됨. (x, y, z) » x<em>y</em>z)<br />
replace = 중복 허용 여부, boolean<br />
p = 1차원 배열, 각 데이터가 선택될 확률<br /></p>

<p>example</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.random.choice(5, 3)
array([0, 3, 4])
# 0 이상 5미만인 정수 중 3개를 출력(replace=True가 default로 숨어져 있으므로 중복 허용)
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.random.choice(5, 3, replace=False)
array([3, 1, 0])
# 0 이상 5미만인 정수 중 3개를 출력(replace=False이므로 중복 불가)
</code></pre></div></div>

<h3 id="flatnonzero">flatnonzero</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
a = np.array([1.2, -1.3, 2.1, 5.0, 4.7])
print(np.flatnonzero(a&gt;2)) *# [2 3 4]*

# 2보다 큰 원소의 index를 array로 리턴.
</code></pre></div></div>

<h3 id="npreshapex_train-x_trainshape0--1">np.reshape(X_train, (X_train.shape[0], -1))</h3>

<p>np.reshape: array를 원하는 형태로 reshape할 수 있게 해주는 라이브러리.</p>

<p>X_train: reshape하고 싶은 array<br />
(X_train.shape[0], -1): X_train이 목표로 하는 모양을 의미합니다.<br />
X_train.shape[0]: X_train array의 row의 수를 의미합니다. <br />
X_train.shape[0]을 이용하여 reshape된 array에서 row의 숫자가 바뀌지 않고 유지되게 됩니다.<br />
-1: 이것은 NumPy가 제공된 행 수와 원래 배열의 전체 크기를 기반으로 재구성된 배열의 열 수를 자동으로 결정하도록 지시하는 자리 표시자 값입니다.<br />
 -1은 차원이 자동으로 추론되고 조정되어야 함을 나타냅니다. 요약하면, 이 코드 라인은 X_train 배열의 형태를 재구성하며, 행 수는 유지되고 원본 배열의 크기에 따라 열 수가 자동으로 결정됩니다.<br /></p>

<h3 id="npzeros">np.zeros</h3>

<p>numpy.zeros(shape, dtype=float, order=’C’, *, like=None)<br />
Return a new array of given shape and type, filled with zeros.<br /></p>

<p>Parameters<br /></p>
<ul>
  <li>shape : int or tuple of ints</li>
  <li>np.zeros((2, 1))</li>
  <li>
    <p>array([[ 0.],
     [ 0.]])</p>
  </li>
  <li>
    <p>dtype : data-type, optional
The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.<br /></p>
  </li>
  <li>
    <p>order : {‘C’, ‘F’}, optional, default: ‘C’<br />
Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.<br /></p>
  </li>
  <li>like : array_like, optional<br />
Reference object to allow the creation of arrays which are not NumPy arrays. If  an array-like passed in as like supports the <strong>array_function</strong> protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.</li>
</ul>

<h3 id="array-인덱싱-이해하기">array 인덱싱 이해하기</h3>

<h4 id="기본-인덱싱">기본 인덱싱</h4>

<p>Numpy array의 꽃은 인덱싱입니다. arr 라는 array가 있을 때, arr[5]와 같이 특정한 인덱스를 명시할 수도 있고, arr[5:8]과 같이 범위 형태의 인덱스를 명시할 수도 있습니다.<br /> 
arr[:]의 경우 해당 array의 전체 성분을 모두 가져옵니다.<br /></p>

<p>이러한 방식은 2차원 array에 대해서도 유사한 방식으로 적용됩니다. arr2라는 2차원 array를 정의한 뒤,
arr2[2, :]를 실행하면, arr2에서 인덱스가 ‘2’에 해당하는 행(3행)의 모든 성분이 1차원 array의 형태로 얻어집니다.<br />
arr2[:, 3]을 실행하면 arr2에서 인덱스가 ‘3’에 해당하는 열(4열)의 모든 성분이 1차원 array의 형태로 얻어집니다.<br /></p>

<p>2차원 array는 이렇게 두개의 인덱스를 받을 수 있는데, “,”를 기준으로 앞부분에는 행의 인덱스가 뒷부분에는 열의 인덱스가 입력됩니다.
arr2[1:3, :] 혹은 arr2[:, :2]와 같이, 행 또는 열에 범위 인덱스를 적용하여 여러 개의 행 혹은 열을 얻을 수도 있습니다.<br /></p>

<p>한편 2차원 array에서 4행 3열에 위치한 하나의 성분을 얻고자 할 때는 arr2[3,2]를 실행하면 됩니다.
인덱싱을 통해 선택한 성분에 새로운 값을 대입하는 경우에도, arr2[:2, 1:3] = 0 과 같이 입력값을 넣으면 됩니다.<br /></p>

<h3 id="numpy-어레이-정렬-npargsort">NumPy 어레이 정렬 (np.argsort)</h3>
<h4 id="기본-사용오름차순-정렬">기본 사용(오름차순 정렬)</h4>

<!--코드블럭-->
<p>** Input **</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np

a = np.array([1.5, 0.2, 4.2, 2.5])
s = a.argsort()

print(s)
print(a[s])

</code></pre></div></div>
<p>** Output **</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 0 3 2]
[0.2 1.5 2.5 4.2]

</code></pre></div></div>

<p>a는 정렬되지 않은 숫자들의 어레이입니다.
a.argsort()는 어레이 a를 정렬하는 인덱스의 어레이 [1 0 3 2]를 반환합니다.
a[s]와 같이 인덱스의 어레이 s를 사용해서 어레이 a를 다시 정렬하면,
오름차순으로 정렬된 어레이 [0.2 1.5 2.5 4.2]가 됩니다.</p>

<h4 id="내림차순-정렬">내림차순 정렬</h4>

<p>** Input **</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np

a = np.array([1.5, 0.2, 4.2, 2.5])
s = a.argsort()

print(s)
print(a[s[::-1]])
print(a[s][::-1])
</code></pre></div></div>
<p>** Output **</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 0 3 2]
[4.2 2.5 1.5 0.2]
[4.2 2.5 1.5 0.2]
</code></pre></div></div>
<p>
내림차순으로 정렬된 어레이를 얻기 위해서는
a[s[::-1]]와 같이 인덱스 어레이를 뒤집어서 정렬에 사용하거나,
a[s][::-1]과 같이 오름차순으로 정렬된 어레이를 뒤집어주면 됩니다.
</p>

<h3 id="reference">Reference</h3>

<p>https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html
<br /></p>]]></content><author><name></name></author><category term="study" /><category term="math" /><category term="numpy" /><category term="math" /><category term="study" /><summary type="html"><![CDATA[recording_a Summary of Numpy]]></summary></entry><entry><title type="html">Computer Vision_Human Pose Estimation</title><link href="https://chojinie.github.io/blog/2023/human-pose-estimation-%EB%B3%B5%EC%82%AC%EB%B3%B8/" rel="alternate" type="text/html" title="Computer Vision_Human Pose Estimation" /><published>2023-05-23T00:00:00+00:00</published><updated>2023-05-23T00:00:00+00:00</updated><id>https://chojinie.github.io/blog/2023/human-pose-estimation%20-%20%EB%B3%B5%EC%82%AC%EB%B3%B8</id><content type="html" xml:base="https://chojinie.github.io/blog/2023/human-pose-estimation-%EB%B3%B5%EC%82%AC%EB%B3%B8/"><![CDATA[<h2 id="글에-들어가기-앞서">글에 들어가기 앞서..</h2>

<p>우선, viso.ai에 잘 정리가 되어 있는 Human Pose Estimation with Deep Learning - Ultimate Overview in 2023을 읽은 것을 시작으로 해당분야의 정리 및 논문 탐색을 해보려 합니다.- <a href="https://viso.ai/deep-learning/pose-estimation-ultimate-overview/">viso.ai/deep-learning 관련 글</a></p>

<p>해당 기고문은 가장 최근의 발전된 pose estimation algorithm과 AI Vision 기술 그리고 이들의 application과 use case 그리고 한계점에 대해 전반적으로 기술하고 있기 때문에 overview에 적합하다고 생각됩니다.</p>

<h2 id="pose-estimation이란">Pose Estimation이란?</h2>

<p>Pose Estimation(이하 ‘P.E’)은 컴퓨터 비전과 AI 분야에서 기본이 되는 task입니다. 대상 이미지나 영상 속에서 묘사되는 인체 부위가 갖는 방향성이나 위치를 감지(Detecting)하고 연결(Associating), 추적(Tracking)하는 것을 포함하고 있습니다.</p>

<p>감지, 연결, 추적의 대상은 Semantic Key point이며 인간의 pose를 estimation할 경우 대표적인 예로 “right shoulders”,”left knees”가 있습니다.</p>

<p>사물의 pose를 estimation할 경우(Object pose estimation) 대표적인 예로 자동차의 “left brake lights of vehicles”가 있습니다.
<img src="https://velog.velcdn.com/images/jinnij/post/10f69d18-0c37-473d-83ea-0c0ba0bf38a5/image.png" alt="Vehicle Pose Estimation using OpenPifPaf" /></p>

<p>라이브 영상에서 semantic keypoint를 추적하게 된다면, P.E의 정확도에 Limit을 걸어도 매우 큰 컴퓨팅 리소스를 필요로 하게 됩니다.</p>

<p>다만, Hardware와 모델 효율성이 최근에 크게 발전함에 따라 real-time 처리의 요구가 있는 새로운 application이 구현 및 경제적 실현이 가능해지고 있는 추세입니다.</p>

<p>최근 이미지 처리 분야에서 가장 powerful한 model은 다수가 CNN(Convolutional Neural Network)를 기반으로 하고 있습니다. 따라서 인간과 물체의 pose를 추론하는 application의 SOTA(State-of-the-Art) method는 CNN Architecture기반으로 이뤄집니다.</p>

<h2 id="bottom-up-vs-top-down-methods">Bottom-up Vs. Top-down methods</h2>

<p>P.E의 모든 접근법은 두 그룹으로 나뉠 수 있습니다.</p>

<ul>
  <li>
    <p>Bottom-up methods
우선 신체에 있는 모든 관절을 estimate한다음, 특정한 포즈 또는 하나의 사람 객체의 포즈로 그룹지어주는 방식이다.
“<strong>DeepCut</strong>모델”이 선도한 영역이라고 합니다.
이 방식의 문제점음 찾은 관절을 매칭할 수 있는 조합이 매우 많고(가령, 사람 1의 팔꿈치를 사람2에 붙일 수도 있겠다) 이를 적절하게 매칭하는데 시간이 많이 걸리며 정확도를 높이는 것이 힘듭니다..
하지만 사람을 먼저 감지하는 과정을 거치지 않기 때문에 Real-time에 적용이 가능합니다.</p>
  </li>
  <li>
    <p>Top-down methods
사람을 먼저 detecting한 후 detecting box 안에 있는 신체 관절을 estimate합니다. 문제점은 사람을 인식하지 못하면 자세 자체를 측정할 수 없고 사람의 수가 많아지면 계산량도 많아집니다.</p>
  </li>
</ul>

<h2 id="pose-estimation의-중요성">Pose Estimation의 중요성</h2>

<p>전통적인 object detection에서 인간은 오직 bounding box로만 인식이 되었습니다.</p>

<p>네모 박스는 그저 인간의 위치만 파악할 뿐 해당 인간이 어떠한 의도가 있는지, 자세는 어떤지, 어떠한 상황인지를 파악하기 어렵죠.</p>

<p>그렇기 때문에 Pose를 detection하고, tracking함으로서 컴퓨터는 인간의 body language를 더욱 잘 이해할 수 있게 되는 것 입니다.</p>

<p>그러나 기존의 pose tracking 방법은 Occlusion을 이겨낼 만큼 robust하지도, fast하지도 않은 한계가 존재했습니다.</p>

<p>그럼에도 최근에는 고성능의 real-time pose detection과 tracking이 비전 분야에서 메가 트렌드로 자리매김하고 있습니다. 예를 들어, 인간의 pose를 real-time으로 추적하면 컴퓨터가 인간 행동을 보다 세밀하고 자연스럽게 이해 할 수 있게 됩니다.
<img src="https://velog.velcdn.com/images/jinnij/post/98f23b79-f6de-4319-abf7-8dd58c8c9f96/image.png" alt="Crowd pose estimation with multi-instance analysis" /></p>

<p>자율 주행, 스포츠, 헬스케어 등등 많은 산업군에 영향을 미칠 수 있게 되는 것입니다. 오늘날 자율 주행 자동차의 사고(사실 진정한 의미의 자율주행은 아니죠. 산업계에서 자율주행차라고 지칭하는 것으로 이해하겠습니다.)의 대부분은 테슬라의 오토파일럿과 같이, 기술이 미성숙한 상태에서 인간이 완전히 기계를 믿기에 발생하는 건이죠. 
만약 인간의 pose를 detection하고 tracking할 수 있습니다면, 컴퓨터는 보행자나 in cabin의 행동을 더욱 이해하여 안전하고 natural한 driving을 할 수 있을 것입니다.
<img src="https://velog.velcdn.com/images/jinnij/post/fb617303-6d54-407e-8068-ab56291dc46b/image.png" alt="" /></p>

<h2 id="human-pose-estimation">Human Pose Estimation</h2>

<p>이제는 P.E의 구체적이 예시를 깊이 들여다 보겠습니다..
Human P.E는 이미지나 비디오 상에 나오는 신체나 관절의 pose를 예측하는 것을 주 목적으로 합니다. pose의 모션은 인간의 특정한 행동에 의해 발생되기 때문에 신체의 pose를 이해하는 것은 동작인식과 video 상의 인간의 동작을 이해하는 <a href="https://viso.ai/deep-learning/pytorchvideo-video-understanding/">Video understanding</a> 분야에서 매우 중요한 문제입니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/4530e87a-7183-4371-b4a8-6f6de56af880/image.png" alt="" /></p>

<h2 id="2d-human-pose-estimation이란">2D Human Pose Estimation이란?</h2>

<p>인체의 keypoint의 2D 위치 또는 공간 위치를 추정하는 것을 의미합니다. 전통적인 2D Human P.E 방법은 개별 신체 부위에 대해 각기 다른 수작업 특징 추출 기술을 사용하였습니다.</p>

<p>초기 컴퓨터 비전 작업은 global pose structure를 얻기 위해 인간의 신체를 막대기 모양으로 묘사했습니다. 그러나 최신 딥러닝 기반 접근 방식은 single-person과 multi-person의 P.E 분야에서 모두 성능을 크게 개선하였습니다.</p>

<p>대표적인 2D Human P.E 기법으로는 OpenPose, CPN, AlphaPose, HRNet이 있습니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/8d3ce8a4-966e-43cb-a34b-20a7b37c494c/image.gif" alt="" /></p>

<h2 id="3d-human-pose-estimation이란">3D Human Pose Estimation이란?</h2>

<p>3D 공간 상에서 신체 관절의 위치를 예측하는 것을 의미합니다. 게다가 3D 포즈 외에도 일부 방법은 이미지 또는 비디오에서 3D 인간 메쉬를 복구합니다. 이 분야는 인체의 광범위한 3차원 구조 정보를 얻는 데에 활용되기 때문에 최근 많은 관심을 받고 있습니다.</p>

<p>다양한 분야에 사용될 수 있는데, 3D 애니메이션 분야, VR/AR, 3D 행동 예측 분야 등이 대표적인 예이다. 3D Human P.E는 monocular 이미지 또는 비디오에서 수행될 수 있습니다.</p>

<p>multiple viewpoint 혹은 IMU, Lidar와 같은 센서를 사용하는 퓨전 기술을 적용한 3D P.E는 매우 어려운 작업이라고 합니다. 2D Human dataset은 얻기가 쉬운 반면, 3D는 상대적으로 어렵습니다. 3D pose의 정확한 image annotaion은 너무 많은 시간이 소요되며, 수동 레이블링(manual labeling)은 실용적이지도 않고 비용이 많이 듭니다.</p>

<p>그래서 비록 3D P.E이 최근 몇년 간 2D P.E의 진전으로 인하여 상당한 발전을 이뤘지만 극복해야할 몇 가지 산이 있습니다고 합니다.
대표적 예시는 아래와 같습니다.</p>
<ul>
  <li><strong>Model generalization</strong></li>
  <li><strong>robustness to occlusion</strong></li>
  <li><strong>computation efficiency.</strong></li>
</ul>

<p>실시간 3D Human P.E를 위해 Neural Network를 사용한 대표적인 라이브러리로는 OpenPose가 있습니다.</p>

<h2 id="3d-human-body-modeling">3D Human Body Modeling</h2>

<p>human P.E에서 인체 부위의 위치는 시각적 입력 데이터로부터 인체 표현(ex: skeleton pose)을 구축하는 데에 사용됩니다. 즉, 시각적 입력 데이터에서 추출한 특징 및 키포인트를 나타내는 데에 사용됩니다. 그렇기에 Human body Modeling이 human P.E에서 중요한 것입니다.</p>

<p>일반적으로 모델 기반의 접근 방식은 인체 pose를 설명 및 추론하고 2D 혹은 3D 포즈를 렌더링 하는 데에 사용됩니다.</p>

<p>대부분의 방법은 N-joint kinematic model을 사용합니다. 이 모델은 인체를 신체 운동학적 구조와 체형 정보를 포함하는 관절과 팔다리가 있는 개체로 표현합니다.</p>

<p>Body Modeling에는 3가지 타입이 있습니다.</p>

<h3 id="kinematic-model">Kinematic Model</h3>
<p>Skeleton-based model이라고도 불리며, 2D와 3D P.E에 사용됩니다. 직관적인 바디 모델로서, 관절의 위치나 팔다리(사지)의 방향과 같이 인체 구조를 나타내는 핵심 정보를 포함하고 있습니다.</p>

<p>이에, 서로 다른 신체 부위 간의 관계를 포착하는 데에 활용되기도 합니다. 하지만, 해당 모델은 Texture 혹은 shape에 관한 정보를 표현하는 데에 부족하다는 한계점이 있습니다.</p>

<h3 id="planar-model">Planar Model</h3>
<p>Contour-based model이라고도 불리며, 2D P.E에 사용됩니다. 이는 인체의 shape과 appearance를 나타내는 데에 사용됩니다. 주로, 인체의 외곽선을 따라 내부를 최대한 꽉꽉 채우는 작은 직사각형들로 인체의 부위를 표현합니다.</p>

<p>대표적인 모델로 Active Shape Model(ASM)이 있으며, Principle component analysis(주성분 분석)을 이용하여 전체 인체 그래프와 실루엣 변형을 포착하는 데에 활용됩니다. 이는 60,000개 이상의 고해상도의 전신 스캔 데이터셋에서 훈련된 완전히 훈련 가능한 end-to-end 딥러닝 파이프라인입니다. 이를 통해 통계적이며, 관절로 분절된 3D 인체 shape과 pose을 모델링 할 수 있습니다.</p>

<h3 id="volumetric-model">Volumetric Model</h3>

<p>3D P.E에 사용됩니다. 3D human mesh 복구를 위한 딥러닝 기반의 3D human P.E에 사용 되는 몇몇 인기 있는 3D 신체 모델이 있습니다. 대표적인 예로 GHUM &amp; GHUML(ite)이 있습니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/937187fc-dc22-407d-9529-f18a23dcf277/image.png" alt="" /></p>

<h2 id="pose-detection의-주요-챌린지">Pose Detection의 주요 챌린지</h2>

<p>Human P.E은 신체의 외형이 계속해서 변해가기 때문에 매우 어려운 Task중 하나입니다. 옷의 형태, randomic하게 occlusion되는 경우, 시야각, 배경의 문맥(상황이라고 이해해본다) 등 많은 요소로 인해 변화가 일어납니다. 나아가 P.E는 빛이나 날씨와 같은 리얼 세계의 수많은 변화에 대응할 수 있을만큼 robust해져야 한다는 도전 과제가 쌓여 있습니다.</p>

<h2 id="head-pose-estimation">Head Pose Estimation</h2>

<p>인간 머리의 pose를 estimate하는 것은 유명한 computer vision 문제입니다. 여기에는 여러 application이 활용되는데 aiding in gaze estimation, modeling attention, fitting 3D models to video, performing face alignment 등이 대표적입니다.</p>

<p>전형적으로 머리 pose를 찾을 때는 대상 얼굴의 keypoint를 사용하고 2D에서 3D pose로의 대응 문제는 mean human head model로 해결합니다.</p>

<p>머리의 3D pose 리커버 능력은 딥러닝 기법을 활용한 2D facial keypoint 추출에 기반한 keypoint-based 표정 분석으로 부터 나오게 됩니다. 해당 방식을 통해 occlusion과 다양한 포즈 변화에 강건한 형태가 되었습니다.</p>

<h2 id="animal-pose-estimation">Animal Pose Estimation</h2>

<p>대부분 SOTA를 달성한 기법들은 인체 포즈에만 관심이 있지만, 몇몇 모델은 동물과 자동차(사물)을 Estimation하기 위해 개발되어 왔습니다.</p>

<p>Human E.P와 달리 추가적인 어려움이 존재하는데, 제한된 라벨 데이터(데이터를 수집하고 수동으로 이미지에 주석을 추가해야하는 등)와 너무도 많은 self-occlusion이 문제입니다. 그래서 동물 포즈 추정을 위한 데이터셋은 아주 제한된 숫자의 동물 종을 포함하고 있습니다.</p>

<p>이렇게 사용 가능한 데이터가 한정적이고 작은 데이터셋으로 작업을 수행할 때는 Active learning과 data augmentation 기법이 필요로 하게 됩니다. 양 기술은 vision 알고리즘을 더욱 효과적으로 학습시키고 맞춤형 AI model 학습을 위한 annotation 작업량을 감소시켜 줍니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/2361edcb-789c-4291-9a87-8a7d93d8df7b/image.png" alt="" /></p>

<p>또한, 다수의 동물의 포즈를 추정하는 것은 어려운 computer vision 문제입니다. 동물간 빈번한 상호작용으로 인해 occlusion을 야기하고 감지된 keypoint를 올바르게 객체마다 할당하는 것을 복잡하게 만들기 때문입니다. 나아가 인간이 볼 때, 매우 비슷하게 생긴 동물이 통상 인간 세계의 상호 작용보다 더욱 긴밀하게 상호작용한다면 이때의 다수 동물의 포즈를 추정하는 것도 어렵겠습니다.</p>

<p>이러한 문제를 해결하기 위해 인간에서 동물로 방법을 다시 적용하는 transfer learning 기술이 개발 되었습니다. 대표적인 예시로 다수의 동물의 포즈를 추정하고 추적할 때 DeepLabCut을 사용합니다. 이는 인간과 동물의 포즈를 추정하는 오픈 소스 툴 박스이며 SOTA를 찍는 모델입니다.</p>

<p>동물 포즈 추적에 관련한 컴퓨터 비전 기술의 응용은 아래 글을 참고하면 됩니다.
<a href="https://viso.ai/applications/computer-vision-in-agriculture/">Computer vision in agriculture</a></p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/4404a68c-5472-4485-bd63-ea9f32693206/image.gif" alt="" /></p>

<h2 id="video-person-pose-tracking">Video Person Pose Tracking</h2>

<p>복잡한 상황에서의 Multi-frame human pose estimation은 매우 복잡하고 높은 컴퓨팅 파워를 요합니다. 인간의 관절 detector가 정적 이미지에서는 좋은 성능을 발휘하는 반면에, ML 모델이 실시간 포즈 추적을 위해 비디오 시퀀스에 적용될 때는 성능이 종종 부족해집니다.</p>

<p>가장 큰 문제들 중 일부는 motion blur 처리, video defocus, pose occlusion, 비디오 프레임 간의 시간적 종속성을 포착할 수 없는 등의 어려움이 있습니다.</p>

<p>기존의 RNN을 적용하면, 특히 pose occlusion을 처리하는데에 공간의 컨텍스트를 모델링하는 데에 경험적인 어려움이 발생하게 됩니다. multi-frame에서 인간의 자세를 추정하는 SOTA를 찍는 프레임워크인 “DCPose”는 비디오 프레임 사이의 풍부한 temporal 단서를 활용하여 eypoint detection을 용이하게 합니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/8b2c95f7-ceeb-48fb-84fe-2be7b166ce9a/image.gif" alt="" /></p>

<h2 id="pose-estimation은-어떻게-동작할까">Pose Estimation은 어떻게 동작할까?</h2>

<p>대부분의 포즈 추정기(pose estimator)는 2단계의 프레임워크로 구성되어 있습니다. 인간의 bounding box를 감지한 다음 각 box내에서 포즈를 추정합니다.</p>

<p>포즈 추정은 사람이나 사물의 keypoint를 찾는 방식으로 작동합니다. 예를 들어 사람의 팔꿈치, 무릎, 손목 등과 같은 관절이 핵심 포인트가 됩니다.</p>

<p>보편적인 MS COCO 데이터셋에서의 인간 자세 추정은 17개의 서로 다른 keypoint(class)를 감지할 수 있습니다. 각 키포인트에는 세개의 숫자(x,y,v)가 annotated됩니다. x,y는 target point의 좌표이고, v는 키포인트인지 아닌지 여부를 나타냅니다.
(key : v=1, non-key : v=0)</p>

<p>“nose”, “left_eye”, “right_eye”, “left_ear”, “right_ear”, “left_shoulder”, “right_shoulder”, “left_elbow”, “right_elbow”, “left_wrist”, “right_wrist”, “left_hip”, “right_hip”, “left_knee”, “right_knee”, “left_ankle”, “right_ankle”</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/dffae4c9-d171-4b71-82ae-bbfe5b74a6ee/image.png" alt="" /></p>

<h3 id="딥러닝-기반의-pose-esimation">딥러닝 기반의 Pose Esimation</h3>

<p>딥러닝의 최근 급격한 발전으로 인해 기존 컴퓨터 비전 분야에서 image segmentation / object detection 분야의 좋은 성과를 내왔다. 그 결과 P.E 분야에도 좋은 성과를 낼 수 있었습니다.</p>

<p><img src="https://velog.velcdn.com/images/jinnij/post/6a5939d5-7c39-49f2-ae10-3cd3f174c2cf/image.png" alt="" /></p>

<h3 id="가장-유명한-pose-estimation-기법">가장 유명한 Pose Estimation 기법</h3>

<ul>
  <li>Method #1: OpenPose</li>
  <li>Method #2: High-Resolution Net (HRNet)</li>
  <li>Method #3: DeepCut</li>
  <li>Method #4: Regional Multi-Person Pose Estimation (AlphaPose)</li>
  <li>Method #5: Deep Pose</li>
  <li>Method #6: PoseNet</li>
  <li>Method #7: Dense Pose</li>
</ul>

<h3 id="상기-기법들의-설명">상기 기법들의 설명</h3>

<p>포즈 추정은 비교적 쉽게 적용해볼 수 있는 비전 기술이기 때문에, 기존 아키텍처를 사용하여 맞춤형 포즈 추정기를 구현해볼 수 있습니다. 이를 위한 기존 아키텍쳐는 아래와 같습니다.</p>

<ol>
  <li>
    <p>OpenPose
가장 유명한 bottom-up 방식의 multi-person human pose estimation 기법입니다. 신체, 발, 손 및 얼굴 키포인트를 높은 정확도로 감지하는 오픈 소스 실시간 다중 사람 감지가 가능합니다. OpenPose API의 장점은 사용자가 카메라 필드, 웹캠 등에서 소스 이미지를 선택할 수 있는 유연성을 제공한다는 점입니다.(예를 들어 CCTV 카메라와 시스템과의 통합) CUDA GPU, OpenCL GPU 또는 GPU 전용 장치와 같은 다양한 하드웨어 아키텍쳐를 지원합니다. 경량 버전은 Edge device에서 실시간으로 온디바이스 프로세싱이 가능한 edge 추론 어플리케이션으로 충분히 사용 가능합니다.</p>
  </li>
  <li>
    <p>High-Resolution Net(HRNet)
인간 포즈 추정을 위한 neural network이다. 이미지에서 특정 개체 또는 사람과 관련하여 키 포인트(관절)로 알고 있는 것을 찾기 위해 이미지 처리애 사용되는 아키텍쳐입니다. 다른 아키텍처에 비해 이 아키텍처의 장점은 대부분의 기존 방법이 고해상도 네트워크 사용과 관련하여 저해상도 표현에서 자세의 고해상도 표현과 일치한다는 점입니다. 이 bias 대신, 신경망은 자세를 추정할 때 고해상도 표현을 유지합니다. 예를 들어, TV 스포츠에서 사람의 자세를 감지하는 데에 도움이 됩니다.</p>
  </li>
  <li>
    <p>DeepCut
또 다른 bottom-up 방식의 multi-person human P.E기법입니다. 이미지 상에서 사람의 수를 감지한 다음, 각 이미지마다의 관절의 위치를 예측합니다. DeepCut은 가령 비디오 상에서의 농구, 축구 등 스포츠나 기타 상황에서 인간과 물체를 estimating하는 데에 적용할 수 있습니다.</p>
  </li>
  <li>
    <p>Regional Multi-Person Pose Estimation(AlphaPose)
유명한 top-down 방식의 P.E이다. 부정확한 인간 Bounding box가 있을 때 포즈를 감지하는 데에 유용합니다. 즉 최적으로 검출된 바운딩 박스를 통해 인간 포즈를 추정하기 위한 최적의 아키텍쳐입니다. 이미지나 영상에서 싱글 혹은 다수의 사람의 포즈를 검출하는 데에 사용될 수 있습니다.</p>
  </li>
  <li>
    <p>DeepPose
Deep Neural network를 활용한 human P.E이다. 모든 관절을 포착하고 pooling layer, convolution layer, fully-connected layer를 연결하여 계층의 일부를 형성합니다.</p>
  </li>
  <li>
    <p>PoseNet
브라우저나 모바일 장치와 같은 가벼운 장치에서 실행하기 위해 tensorflow.js에 구축된 포즈 추정기 아키텍처입니다. 따라서 PoseNet를 사용하여 단일 혹은 여러 포즈를 추정할 수 있습니다.</p>
  </li>
  <li>
    <p>DensePose
RGB이미지의 모든 인간 픽셀을 인체의 3D 표면에 매핑하는 것을 목표로하는 P.E 기법입니다. 단일/다수의 P.E에 적용할 수 있습니다.</p>
  </li>
  <li>
    <p>TensorFlow Pose Estimation</p>
  </li>
  <li>
    <p>OpenPifPaf
P.E를 위한 오픈 소스 형 컴퓨터 비전 라이브러리 및 프레임워크입니다. 이미지 또는 비디오에서 인체 부위를 식별하고 위치를 파악하는 것과 관련이 있습니다. PyTorch 딥 러닝 프레임워크 위에 구축되었으며 multi-task learning 접근 방식을 사용하여 정확하고 효율적인 포즈 추정을 합니다. 사용성이 좋으며 강건성, 어려운 P.E 시나리오(가령, occlusion, cluttered background)를 다룰 수 있기 때문에 유명해졌습니다.
<img src="https://velog.velcdn.com/images/jinnij/post/72ac558c-1449-4181-bfbc-c7f6a8423b20/image.png" alt="" /></p>
  </li>
</ol>

<h2 id="use-cases--applications-of-pose-estimation">Use Cases &amp; Applications of Pose Estimation</h2>

<h3 id="가장-유명한-pose-estimation-applications">가장 유명한 Pose Estimation applications</h3>

<p>다양한 분야에서 사용됩니다 가령. <strong>Human-computer interaction(HCI), Action Recognition, Motion Capture, Movement analysis, Augmented reality, Sports and Fitness, Robotics.</strong></p>

<ul>
  <li>Application #1: Human Activity Estimation</li>
  <li>Application #2: Motion Transfer and Augmented Reality</li>
  <li>Application #3: Motion Capture for Training Robots</li>
  <li>Application #4: Motion Tracking for Consoles</li>
  <li>Application #5: Human Fall Detection
<img src="https://velog.velcdn.com/images/jinnij/post/e147972f-2e55-4bd0-bb4b-9d7d8059966b/image.png" alt="" /></li>
</ul>

<h3 id="상기-applications의-설명">상기 applications의 설명</h3>

<ol>
  <li>Human Activity Estimation
확실한 사용처는 인간의 활동과 움직임을 추적하고 측정하는 것입니다. DensePose, PoseNet, OpenPose는 종종 활동, 제스처 또는 보행 인식에 사용됩니다. 자세 추정을 통한 인간 활동 추적의 예는 다음과 같습니다.</li>
</ol>

<ul>
  <li>
    <p>앉은 제스처 감지, 손 제스처 인식 또는 얼굴 표정 분석을 위한 애플리케이션</p>
  </li>
  <li>운동 선수에 대한 AI 기반 분석</li>
  <li>댄스 기술 분석을 위한 애플리케이션(예: 발레)</li>
  <li>의료 수술 서비스의 품질을 평가하기 위한 컴퓨터 비전 시스템</li>
  <li>운동의 실행 형태를 감지학 반복 횟수를 카운트하는 피트니스 어플리케이션</li>
  <li>전신/ 수화 커뮤니케이션(예 : 교통 경찰 신호)</li>
  <li>넘어지는 사람의 감지 혹은 특정 질병의 진행과정을 감지하는 지능형 어플리케이션
<img src="https://velog.velcdn.com/images/jinnij/post/83ea2af8-da41-431e-ba73-1af9306c350f/image.png" alt="" /></li>
</ul>

<ol>
  <li>Augmented Reality and Virtual Reality
현재 AR/VR에 반영된 P.E기술은 사용자들에게 더 나은 온라인 경험을 제공하고 있습니다. 예를 들어 사용자는 포즈를 취하는 가상 튜터를 통해 테니스를 배우는 사례가 있습니다.</li>
</ol>

<p>또한 포즈 추정기는 증강 현실 기반 응용프로그램과 인터페이스할 수도 있습니다. 예를 들어 미 육군은 전투에 사용할 증강 현실 프로그램을 실험하기도 합니다. 이 프로그램은 병사들이 피아식별을 하고 야간 시력을 향상시키는 것을 목표로 한다고 합니다.
<img src="https://velog.velcdn.com/images/jinnij/post/bb71aae4-0fbb-4a3c-95cf-b0b50f25a9a6/image.png" alt="" /></p>

<ol>
  <li>Training Robots with Human Pose Tracking</li>
</ol>

<p>포즈 추정기의 일반적인 사용 사례는 로봇이 특정 기술을 배우도록 하는 어플리케이션에 있습니다. 궤적을 따르도록 수동으로 로봇을 프로그래밍하는 대신 로봇이 교사의 자세, 외모나 외형을 따라하여 행동을 배우도록 할 수 있습니다.</p>

<ol>
  <li>Human Motion Tracking for Consoles
게임 내 응용 프로그램으로도 사용할 수 있습니다. 인간이 대화형 게임 경험을 위해 게임 환경에 포즈를 자동 생성하고 주입하게 됩니다. 가령, Microsoft는 3D 포즈 추정(IR Sensor사용)을 사용하여 인간 플레이어의 동작을 추정하고 이를 사용하여 캐릭터의 동작을 게임 환경에 가상으로 렌더링했습니다.</li>
</ol>

<h1 id="reference">Reference</h1>

<p>https://ctkim.tistory.com/101<br />
viso.ai<br /></p>]]></content><author><name></name></author><category term="study" /><category term="computer_vision" /><category term="Human_Pose_Estimation" /><summary type="html"><![CDATA[recording_a Summary of a HPE]]></summary></entry></feed>